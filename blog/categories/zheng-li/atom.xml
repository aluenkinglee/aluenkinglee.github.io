<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 整理 | 曾经渐行远，未免心戚戚]]></title>
  <link href="http://aluenkinglee.com/blog/categories/zheng-li/atom.xml" rel="self"/>
  <link href="http://aluenkinglee.com/"/>
  <updated>2014-07-14T14:08:43+08:00</updated>
  <id>http://aluenkinglee.com/</id>
  <author>
    <name><![CDATA[Aluen King Lee]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[generative method]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/30/generative-method/"/>
    <updated>2014-04-30T22:52:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/30/generative-method</id>
    <content type="html"><![CDATA[<p>线性回归模型和logistic回归是判别模型，也就是根据特征值来求结果的概率。形式化表
示为<script type="math/tex">p(y|x;\theta)</script>在参数<script type="math/tex">\theta</script>确定的情况下，求解条件概率<script type="math/tex">p(y|x)</script>。
通俗的解释为在给定特征后预测结果出现的概率。</p>

<p>就按照Andrew Ng讲的那样，确定肿瘤是良性的还是恶性的，可以使用判别模型的方法是先
从历史数据中学习到模型，然后通过提取肿瘤的特征来预测出它是良性恶性的概率。</p>

<p>反过来，要是我们先从良性肿瘤学习出良性肿瘤的模型，从恶性肿瘤学习出恶性肿瘤的模型，
然后提取肿瘤的特征，放到良性肿瘤的模型看下概率，在放到恶性肿瘤的模型看下概率，哪个
大是哪个。</p>

<p>形式化表示为求<script type="math/tex">P(X|Y)</script>,x是特征，y是类型即模型。
利用贝叶斯公式发现两个模型的统一性：</p>

<script type="math/tex; mode=display">
p(y|x)=\frac { p(x|y)p(y) }{ p(x) } 
</script>

<p>由于我们关注的是 y 的离散值结果中哪个概率大（比如良性肿瘤和恶性肿瘤哪个概率大），
而并不是关心具体的概率，因此上式改写为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\quad \quad \quad \quad \begin{eqnarray} \max _{ y }{ p(y|x) }  & = & \max _{ y }{ \frac { p(x|y)p(y) }{ p(x) }  }  \\  & = &\max _{ y } p(x|y)p(y) \end{eqnarray}
 %]]&gt;</script>

<table>
  <tbody>
    <tr>
      <td>其中$$p(x</td>
      <td>y)<script type="math/tex">称为后验概率,</script>p(y)$$称为先验概率.</td>
    </tr>
  </tbody>
</table>

<p>由<script type="math/tex">p(x|y)*p(y)=p(x,y)</script>,因此有时称判别模型求的是条件概率，生成模型求的是联
合概率。</p>

<p>常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件
随机场、神经网络等。</p>

<p>常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted 
Boltzmann Machine 等。</p>

<p>上篇博客较为详细地介绍了两个模型</p>

<h3 id="gaussian-discriminant-analysis">高斯判别分析（Gaussian discriminant analysis）</h3>

<h5 id="section">多维正太分布</h5>

<p>多变量正态分布描述的是n维随机变量的分布情况。所以这里的<script type="math/tex">\mu </script>变成了n维随机变量，<script type="math/tex">\sigma </script>也变成了
矩阵<script type="math/tex">\Sigma </script>.记做<script type="math/tex">N(\mu,\Sigma)</script>.假设有 n 个随机变量<script type="math/tex">X_1,X_2,\cdots ,X_n</script>.所以显而易见，<script type="math/tex">\mu </script>的第i个分量是<script type="math/tex">E(X_i),\Sigma_{ii}=Var(X_i),\Sigma_{ij}=Cov(X_i,X_j)</script>.</p>

<p>概率密度函数如下：</p>

<script type="math/tex; mode=display">
p(x;\mu,\Sigma)=\frac { 1 }{ \left( 2\pi  \right) ^{ n/2 }\left| \Sigma  \right| ^{ 1/2 } } exp\left( -\frac { 1 }{ 2 } \left( x-\mu \right)^T \Sigma^{-1}{\left(x-\mu\right)} \right) 
</script>

<h5 id="section-1">模型分析与应用</h5>

<table>
  <tbody>
    <tr>
      <td>如果输入特征x连续型随机变量，那么可以使用高斯判别分析模型来确定$$p(x</td>
      <td>y)$$。模型如下,先以二元分布即伯努利分布来说（因为前面的例子是二元的）:</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">
y\sim Bernoulli\left( \phi \right) \\
x|y=0\sim N(\mu_0,\Sigma)\\
x|y=1\sim N(\mu_1,\Sigma)
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Discriminative Model Vs. Generative Model]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/30/discriminative-model-vs-generative-model/"/>
    <updated>2014-04-30T17:17:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/30/discriminative-model-vs-generative-model</id>
    <content type="html"><![CDATA[<h2 id="section">判别模型和生成模型分析</h2>

<p>在学习复习ML内容时，中文检索生成模型搜到了该<a href="http://blog.sciencenet.cn/home.php?mod=space&amp;uid=248173&amp;do=blog&amp;id=227964">这里</a>本文主要参考该文章，并稍作整理。</p>

<!-- more -->

<p>这两者进行预测的方式不同在于模型的处理上：</p>

<p><strong>生成模型</strong>：无穷样本 ==&gt; 概率密度模型 = 产生模型 ==&gt; 预测</p>

<p><strong>判别模型</strong>：有限样本 ==&gt; 判别函数 = 预测模型 ==&gt; 预测</p>

<p>简单的说，假设<script type="math/tex">x</script>是观察值，<script type="math/tex">y</script>是模型。</p>

<p>如果对先验概率<script type="math/tex">P(x|y)</script>建模，就是<strong>生成模型（Generative modle）</strong>。
其基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大。
这种方法一般建立在统计力学和bayes理论的基础之上。</p>

<p>如果对条件概率(后验概率)<script type="math/tex">P(y|x)</script>建模，就是<strong>判别模型（Discrminative modle）</strong>。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。代表性理论为统计学习理论。
这两种方法目前交叉较多。</p>

<h3 id="discriminative-model">判别模型Discriminative Model</h3>

<p>又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)，即<script type="math/tex"> p(class|context)</script>。按照上文的记法就是<script type="math/tex">P(y|x)</script>
利用正负例和分类标签，焦点在判别模型的边缘分布。目标函数直接对应于分类准确率。</p>

<h6 id="section-1">主要特点：</h6>

<p>寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p>

<h6 id="section-2">优点:</h6>

<ol>
  <li>
    <p>分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。</p>
  </li>
  <li>
    <p>能清晰的分辨出多类或某一类与其他类之间的差异特征</p>
  </li>
  <li>
    <p>在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好</p>
  </li>
  <li>
    <p>适用于较多类别的识别</p>
  </li>
  <li>
    <p>判别模型的性能比生成模型要简单，比较容易学习</p>
  </li>
</ol>

<h6 id="section-3">缺点:</h6>

<ol>
  <li>
    <p>不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。</p>
  </li>
  <li>
    <p>Lack elegance of generative: Priors, 结构, 不确定性</p>
  </li>
  <li>
    <p>Alternative notions of penalty functions, regularization, 核函数</p>
  </li>
  <li>
    <p>黑盒操作: 变量间的关系不清楚，不可视</p>
  </li>
</ol>

<p><strong>常见的机器学习方法</strong>：</p>

<ol>
  <li>
    <p>logistic regression</p>
  </li>
  <li>
    <p>支持向量机（SVM）</p>
  </li>
  <li>
    <p>传统的神经网络（traditional neural networks）</p>
  </li>
  <li>
    <p>K近邻，最近邻（Nearest neighbor）</p>
  </li>
  <li>
    <p>Conditional random fields(CRF): 目前最新提出的热门模型，从NLP领域产生的，正在向ASR和CV上发展。</p>
  </li>
</ol>

<h6 id="section-4">主要应用：</h6>

<ol>
  <li>
    <p>图像文本分类</p>
  </li>
  <li>
    <p>生物科学分析</p>
  </li>
  <li>
    <p>时间序列预测</p>
  </li>
</ol>

<h3 id="generative-model">生成模型Generative Model</h3>

<table>
  <tbody>
    <tr>
      <td>估计的是联合概率分布（joint probability distribution），$$p(class, context)=p(class</td>
      <td>context)*p(context)<script type="math/tex">,换用之前的描述就是</script>p(y, x)=p(y</td>
      <td>x)*p(x)$$.</td>
    </tr>
  </tbody>
</table>

<p>用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的draw建模），或作为生成条件概率密度函数的中间步骤。通过使用<strong>贝叶斯定律</strong>可以从生成模型中得到条件分布。</p>

<p>如果观察到的数据是完全由生成模型所生成的，那么就可以拟合生成模型的参数，从而仅可能的增加数据相似度。但观测数据往往完全从生成模型得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。</p>

<h6 id="section-5">主要特点:</h6>

<ol>
  <li>
    <p>一般主要是对<strong>后验概率</strong>建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度</p>
  </li>
  <li>
    <p>只关注自己的inclass本身（即点左下角区域内的概率），不关心到底判别边界在哪</p>
  </li>
</ol>

<h6 id="section-6">优点:</h6>

<ol>
  <li>
    <p>模型可以通过增量学习得到（同意！）</p>
  </li>
  <li>
    <p>研究单类问题比判别模型灵活性强(怀疑？)</p>
  </li>
  <li>
    <p>能用于数据不完整（missing data）情况(怀疑？)</p>
  </li>
  <li>
    <p>实际上带的信息要比判别模型丰富（多太多！同意！）</p>
  </li>
  <li>
    <p>prior knowledge can be easily taken into account（同意！）</p>
  </li>
  <li>
    <p>modular construction of composed solutions to complex problems</p>
  </li>
  <li>
    <p>robust to partial occlusion and viewpoint changes</p>
  </li>
  <li>
    <p>can tolerate significant intra-class variation of object appearance</p>
  </li>
</ol>

<h6 id="section-7">缺点:</h6>

<ol>
  <li>学习和计算过程比较复杂</li>
</ol>

<p><strong>常见的机器学习方法</strong>：</p>

<ol>
  <li>
    <p><strong>Gaussians判别分析</strong></p>
  </li>
  <li>
    <p>** Naive Bayes**， Bayesian networks</p>
  </li>
  <li>
    <p>Mixtures of Gaussians（混合高斯模型）</p>
  </li>
  <li>
    <p>HMMs，Markov random fields</p>
  </li>
  <li>
    <p>Sigmoidal belief networks</p>
  </li>
</ol>

<h4 id="section-8">两者之间的关系</h4>

<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<p>http://prfans.com/forum/viewthread.php?tid=80</p>

<p>http://hi.baidu.com/cat_ng/blog/item/5e59c3cea730270593457e1d.html</p>

<p>http://en.wikipedia.org/wiki/Generative_model</p>

<p>http://blog.csdn.net/yangleecool/archive/2009/04/05/4051029.aspx</p>
]]></content>
  </entry>
  
</feed>
