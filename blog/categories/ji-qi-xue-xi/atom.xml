<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | 曾经渐行远，未免心戚戚]]></title>
  <link href="http://aluenkinglee.com/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://aluenkinglee.com/"/>
  <updated>2014-07-14T14:08:43+08:00</updated>
  <id>http://aluenkinglee.com/</id>
  <author>
    <name><![CDATA[Aluen King Lee]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[use svm to classify whether a person is good or bad on credit using sns data]]></title>
    <link href="http://aluenkinglee.com/blog/2014/07/12/use-svm-to-classify-whether-a-person-is-good-or-bad-on-credit-using-sns-data/"/>
    <updated>2014-07-12T17:21:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/07/12/use-svm-to-classify-whether-a-person-is-good-or-bad-on-credit-using-sns-data</id>
    <content type="html"><![CDATA[<p>首先说明这只是一个思路方向性的大概说明介绍，更多关于业务方面的内容不方便介绍。</p>

<p>如何评价一个人的金融信用，这个可以搜集用户的一些基本数据，比如职业，大学，社交数据来进行评分判断，至于为什么选取这些特征，一个很显然的理由就是
一个人越倾向使用社交应用，那么这个人就越可能是真实的，可信的。职业学校和人的信用也是成一定关系但不是绝对就是向我们想当然的那样。</p>

<p>之前的文章也断断续续的聊过伪匹配，分类的内容，是和此有一定关联。</p>

<p>对于模型的构建，方式有很多，贝叶斯网络，逻辑回归模型，svm模型，甚至是神经网络模型都可以对此进行建模使用，但是哪个性能更好呢？不知道，只有分别做出之后
比较才可以说明问题。</p>

<!-- more -->

<h4 id="svm">非线性SVM</h4>

<p>前面的blog有讲到线性svm，对于非线性分类器就要把x映射到特征空间,同时考虑误差ε的存在（即有些样本点会越过分类边界），上述优化问题变为：、</p>

<script type="math/tex; mode=display">
\min { \quad \frac { 1 }{ 2 } \left\| w \right\| ^{ 2 }+c\sum _{ i=1 }^{ l }{ { \xi  }_{ i } }  } \\ st.\quad y_{ i }(\omega ^{ T }\phi (x_i)-b)\ge 1-\xi _{ i },\left( \xi _{ i }>0 \right) 
</script>

<p>从输入空间是映射到特征空间的函数称为核函数，LibSVM中使用的默认核函数是RBF（径向基函数radial basis function），即</p>

<script type="math/tex; mode=display">
K(x,y)=exp{ \left( \frac { -\left\| x-y \right\| ^{ 2 } }{ 2\sigma ^{ 2 } }  \right)  }
</script>

<p>这样一来就有两个参数需要用户指定：c和gamma。实际上在LibSVM中用户需要给出一个c和gamma的区间，
LibSVM采用交叉验证cross-validation accuracy的方法确定分类效果最好的c和gamma。</p>

<p>举个例子说明什么是交叉验证，假如把训练样本集拆成三组，然后先拿 1 跟 2 来 train model 并 predict 3 以得到正确率；
再来拿 2 跟 3 train 并 predict 1 ，最后 1,3 train 并 predict 2 ，最后取预测精度最高的那组c和gamma。</p>

<h4 id="libsvm">libsvm</h4>

<p>点击<a href="http://www.csie.ntu.edu.tw/~cjlin/cgi-bin/libsvm.cgi?+http://www.csie.ntu.edu.tw/~cjlin/libsvm+tar.gz">here</a>下载libsvm.</p>

<p>看readme即可使用了。</p>

<p>LibSVM要求处理的文件数据都满足如下格式：</p>

<p><code>text
rlabel1 index1:value1   index2:value2   …...
rlabel2 index1:value1   index2:value2   …...
</code></p>

<p>rlabel表示分类，为一个数字。Index从1开始递增，表示输入向量的序号，value是输入向量相应维度上的值，如果value为0,该项可以不写。下面是一个示例文件：</p>

<p>目前简单的分类所使用的属性有：年龄WOE   类型WOE   信用卡邮箱授权WOE  搜多引擎返回数WOE  贴吧搜索返回数WOE  微博数WOE  活跃度WOE  淘友朋友WOE 人人看过的人数WOE  好友数WOE  微博注册时间WOE
预测值为还款情况（还款1，逾期0）</p>

<p><code>text
0 1:0.010003 2:-0.10524 3:0.131117521 4:0.061095504 5:0.029130009 6:-0.178635093 7:-0.149465309 8:-0.03275873 9:-0.047039163 10:-0.047039163 11:-0.158328769
0 1:0.010003 2:-0.10524 3:0.131117521 4:-0.025995297 5:-0.051360537 6:-0.090148625 7:-0.149465309 8:-0.005168969 9:0.000132475 10:-0.050262447 11:-0.158328769
0 1:-0.08606 2:-0.10524 3:0.131117521 4:-0.163782716 5:-0.051360537 6:-0.178635093 7:-0.149465309 8:-0.03275873 9:-0.047039163 10:-0.047039163 11:-0.158328769
0 1:-0.21088 2:0.009167 3:0.131117521 4:0.061095504 5:0.029130009 6:-0.090148625 7:-0.129577755 8:-0.03275873 9:0.000132475 10:-0.050262447 11:-0.226676962
</code></p>

<p>svm_scale用于把输入向量按列进行规范化（或曰缩放）。</p>

<p><code>bash
Usage: svm-scale [options] data_filename
options:
-l lower : x scaling lower limit (default -1)
-u upper : x scaling upper limit (default +1)
-y y_lower y_upper : y scaling limits (default: no y scaling)
-s save_filename : save scaling parameters to save_filename
-r restore_filename : restore scaling parameters from restore_filename
</code></p>

<p>例如： <code>svm_scale -l 0 -u 1 -s range trainSet &gt; trainSet.scale</code>则输入文件是trainSet，输出文件是trainSet.scale，把输入向量的各列都缩放到[0，1]的范围内，range文件中保存了相关的缩放信息。</p>

<p>这个时候我们应该把训练集分为两部分，训练集和测试集，在训练集上通过交叉验证学到最佳的参数，然后在测试集上验证。</p>

<p>```bash
$python subset.py
Usage: subset.py [options] dataset number [output1] [output2]
This script selects a subset of the given data set.
options:
-s method : method of selection (default 0)</p>

<pre><code> 0 -- stratified selection (classification only)

 1 -- random selection
</code></pre>

<p>output1 : the subset (optional)</p>

<p>output2 : the rest of data (optional)</p>

<p>If output1 is omitted, the subset will be printed on the screen.</p>

<p>$python subset.py trainSet 0.3*m(实例个数) trainSet.data testSet.data
$ ./tools/subset.py ./trainningset.txt 280 testSet trainSet
```</p>

<p>grid.py是一种用于RBF核函数的C-SVM分类的参数选择程序。用户只需给定参数的一个范围，grid.py采用交叉验证的方法计算每种参数组合的准确度来找到最好的参数。</p>

<p><code>python
python grid.py
Usage: grid.py [-log2c begin,end,step] [-log2g begin,end,step] [-v fold]
       [-svmtrain pathname] [-gnuplot pathname] [-out pathname] [-png pathname]
       [additional parameters for svm-train] dataset
The program conducts v-fold cross validation using parameter C (and gamma)= 2^begin, 2^(begin+step), ..., 2^end.
</code></p>

<p>首先<code>sudo apt-get install gnuplot</code></p>

<p>然后编译C++版本的LibSVM，生成svm-train二进制可执行文件。</p>

<p><code>bash
python grid.py -log2c -5,5,1 -log2g -4,0,1 -v 5 -svmtrain /path/to/your/svm-train -m 500 trainSet.data (svm-train 的路径自个找好)
</code></p>

<p>-m 500是使用svm_train时可以使用的参数。
最后输出两个文件：dataset.png绘出了交叉验证精度的轮廓图，dataset.out对于每一组log2(c)和log2(gamma)对应的CV精度值。</p>

<p>得到c=16，g=1</p>

<p>这个是我实验的截图</p>

<p><img src="https://raw.githubusercontent.com/aluenkinglee/mlclass/master/libsvm-3.18/trainSet.png" alt="实验截图" /></p>

<p>最后来训练我们的模型</p>

<p>```bash
$ ./svm_train</p>

<p>-s svm_type : set type of SVM (default 0)
0 – C-SVC
-t kernel_type : set type of kernel function (default 2)
0 – linear: u’<em>v
2 – radial basis function: exp(-gamma</em>|u-v|^2)
-g gamma : set gamma in kernel function (default 1/num_features) num_features是输入向量的个数
-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)
-m cachesize : set cache memory size in MB (default 100) 使用多少内存
-e epsilon : set tolerance of termination criterion (default 0.001) 
-h shrinking : whether to use the shrinking heuristics, 0 or 1 (default 1) 
-wi weight : set the parameter C of class i to weight*C, for C-SVC (default 1) 当各类数量不均衡时为每个类分别指定C
-v n: n-fold cross validation mode交叉验证时分为多少组
-q : quiet mode (no outputs)</p>

<p>$ svm_train -s 0 -c 16 -t 2 -g 1 -e 0.01 trainSet.scale
$ ./svm-train -s 2 -c 16 -g 1 -v 5 ./trainSet </p>

<p>```</p>

<p>会得到训练结果，然后使用这个模型来预测测试集的数据准确性</p>

<p><code>bash
./svm-predict testSet result
./svm-predict -b 0 testSet trainSet.model  result
</code></p>

<h3 id="section">实验结果</h3>

<p>对数据进行格式转换后，首先对数据集进行分割得到训练集和测试集，选择常规比例为6：4</p>

<p>1.一开始未设置最有参数时即C和simga的值时，由训练集得到的模型分类性能在测试集上达到了Accuracy = 79.64285714285714% (223/280)的精度。</p>

<p>2.选择合适的参数。</p>

<p>通过交叉验证方法对模型进行选择得到针对训练集上最优的参数为c=16 g=1</p>

<p>由此得到的训练模型在测试集上的准确率达到了Accuracy = 81.4286% (228/280) (classification)，将近2%的精度提升，也算不错了。</p>

<p>目前由这些属性判断用户的还款情况，准确率在80%左右。但是预测结果全部预测为1.不合理。</p>

<p>实验结果准确率，在指定one-vs-class 之后，准确率为40%。比起逻辑回归模型仍然好点。</p>

<blockquote>
  <blockquote>
    <p>Ref</p>
  </blockquote>
</blockquote>

<p>数据可以到<a href="https://github.com/aluenkinglee/mlclass/tree/master/libsvm-3.18">这里</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[朴素贝叶斯文本分类]]></title>
    <link href="http://aluenkinglee.com/blog/2014/06/27/po-su-bei-xie-si-wen-ben-fen-lei/"/>
    <updated>2014-06-27T13:11:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/06/27/po-su-bei-xie-si-wen-ben-fen-lei</id>
    <content type="html"><![CDATA[<p>回顾朴素贝叶斯(NB)分类器:</p>

<script type="math/tex; mode=display">
p(y_k|x)=\frac{p(y_k)p(x|y_k)}{p(x)}\propto p(y_k)p(x|y_k)=p(y_k)\prod_{i=1}^{d}{p(x_i|y_k)}
</script>

<p>对于文本分类任务,即对一篇文章进行分类,是 NLP 中最常见的机器学习任务。一般情
况下,类别从几个到几十不等,或者更多。使用朴素贝叶斯文分类器进行文本分类,我们需要首先考
虑特征是什么,即x如何表示;<script type="math/tex">p(x_i|y_k )</script>的物理意义是什么,如何计算。
对于特征方面,文本分类常规都是使用 bag-of-words 的特征,即以文章中出现的词作为
特征,而不考虑词语出现的顺序。朴素贝叶斯文分类器也一般使用这种形式。那么特征空间的大小,
就取决于词表(vocabulary)的大小,即语料集合中不重复词的个数。对于汉语来说,一般
几万到百万不等。</p>

<p>在 bag-of-words 的特征体系下,特征空间是确定了的,但是具体<script type="math/tex">x_i</script>的取值以及对应的
<script type="math/tex">p(x_i |y_k )</script>的物理意义却可以有不同的考虑,对应着不同的参数计算公式及分类器训练和预测
的实现。这取决于我们是否考虑词语在文章中出现的频次。</p>

<!-- more -->

<h4 id="bernoullinb">伯努力(Bernoulli)NB</h4>

<p>先看不考虑词频的情况。即只看某词语在某文章中是否出现,而不管出现了具体是多少次。
这种假设下,
每维特征的取值为 0-1,此时对应的 NB 分类器又被称为伯努力(Bernoulli)NB 分类器。
比如,如果词表是{2014、年、巴西、世界杯、足球赛、举行、是、第、20、届、球队},
某文档是“2014 年巴西世界杯足球赛是第 20 届世界杯足球赛 ”,那么特征空间是 11,该文档
特征向量是:</p>

<script type="math/tex; mode=display">
x = (1,1,1,1,1,0,1,1,1,1,0)
</script>

<p>此时,<script type="math/tex">p(x_i|y_k)</script>的物理意义可认为是:若文章为 k 类别,则第 i 特征(词表第 i 个词)出
现或者不出现的概率。那么:</p>

<script type="math/tex; mode=display">
p(x_i = 0|y_k ) = 1 − p(x_i = 1|y_k)
</script>

<p>习惯的,我们经常用<script type="math/tex">p(x_i |y_k )</script>来作为<script type="math/tex">p(x_i = 1|y_k)</script>同等含义的一种表示。那此时,原NB
模型的表达式可以写为:</p>

<script type="math/tex; mode=display">
p\left( { { y }_{ k } }|{ x } \right) \propto 
p\left( { y }_{ k } \right) \prod _{ i=1 }^{ d }{ \left\{ { \alpha  }_{ i }p\left( { { x }_{ i } }|{ { y }_{ k } } \right) 
+\left( 1-{ \alpha  }_{ i } \right) \left( 1-p\left( { { x }_{ i } }|{ { y }_{ k } } \right)  \right)  \right\}  } 
</script>

<p><script type="math/tex">\alpha_i</script> 表示第 i 个词在该文档中出现了,没出现则为0.
此时要非常注意,计算文章属于某个类别的得分的时候,不只要考虑该文章的 word,
还要考虑在词表中的但是在该文章中没出现的 word!这类词对得分的贡献是<script type="math/tex">1 − p(x_i |y_k )</script>。
因此伯努力 NB 下,分类的时间复杂度是 <script type="math/tex">O(Cd)</script>,C 是类别数,d 是词表大小。</p>

<p>那么伯努力 NB 下,p的参数估计表达式是多少呢?假设根据如上定义,及最大似然估
计,可以得到:</p>

<script type="math/tex; mode=display">
p\left( { { x }_{ i } }|{ { y }_{ k } } \right) 
=\frac { \sum _{ t=1 }^{ n }{ I\left\{ y_{ k }={ y }_{t } \right\} I\{ x_{ i }\quad in\quad y_{ t }\}  }  }{ \sum _{ t=1 }^{ t }{ I\left\{ y_{ k }={ y }_{ t } \right\}  }  }
=\frac { 特征词i在第k类文章中出现的文章数 }{ 第k类文章数 } 
</script>

<p>其中函数I是指示函数，若x出现则值为1。可见,对于高频词,对应的这种条件概率是非常高的。 比如“的”(假设没去除停用词),其对应的条件概率值很可能会接近于 1.</p>

<p>再重复强调一下,此时的概率意义约束是:</p>

<script type="math/tex; mode=display">
p(x_i = 1|y_k) + p(x_i = 0|y_k ) = 1
</script>

<p>看一下伯努力 NB 下参数平滑的问题。使用加 1 平滑,即拉普拉斯平滑,此时在保证概
率意义下,其平滑公式应该为:</p>

<script type="math/tex; mode=display">
p(x_i |y_k ) = \frac{第 k 类文章中出现过第 i 词的文章数 + 1}{第k类文章数 + 2}
</script>

<p>提醒一下,此处分母加的值是 2,而不是词表大小。注意,平滑一定要使得平滑之后仍
满足概率意义。</p>

<h4 id="multinomialnb">多项式(Multinomial)NB</h4>

<p>当我们考虑文章内词语的频次,而不只是考虑出现或未出现,此时特征的取值不再
是 0-1,不过总的特征空间大小仍未变化。拿前面的例子来做对照,词表是{2014、年、
巴西、世界杯、足球赛、举行、是、第、20、届、球队},某文档是“2014 年巴西世界
杯足球赛是第 20 届世界杯足球赛”,此时该文章的特征向量为:</p>

<script type="math/tex; mode=display">
x = (1,1,1,2,2,0,1,1,1,1,0)
</script>

<p>此时对应的 NB 一般称为多项式 NB。设 m 为文章内的总词频数,对应的模型表达
式应该如下:</p>

<script type="math/tex; mode=display">
p(y_k|x)∝p(y_k)p(x|y_k)=p(y_k)
\frac{m!}{\prod_{i=1}^{d} x_i!}
\prod_{i=1}^{d}p(w_i|y_k)^{x_i}
∝p(y_k)\prod_{i=1}^{d}p(w_i|y_k)^{x_i}
</script>

<p>之所以可以省掉这个多项式系数,是因为它是和类别<script type="math/tex">y_k</script>无关的。而此时,第 k 类别的所有文章中第 i 词的分布概率:</p>

<script type="math/tex; mode=display">
p\left( { { w }_{ i } }|{ { y }_{ k } } \right)
 =\frac { \sum _{ t=1 }^{ n }{ I\left\{ y_{ k }={ y }_{ t } \right\} x_{ i }^{ t } }  }
{ \sum _{ j=1 }^{ d }{ \sum _{ t=1 }^{ n }{ I\left\{ y_{ k }={ y }_{ t } \right\} x_{ i }^{ t } }  }  } 
=\frac { 特征词i在第k类文章中出现的总词频数 }{ 第k文章总词频数 } 
</script>

<p>在多项式NB下,即使极高频词,其
<script type="math/tex">p(w_i|y_k)</script>也很难接近于 1,
另外其在模型中作用的时候是:<script type="math/tex">p(w_i|y_k)^{x_i}</script>.这时候的概率约束是:</p>

<script type="math/tex; mode=display">
\sum_{i=1}^{d}p(w_i|y_k)=1
</script>

<p>因此对应的加 1 平滑为:</p>

<script type="math/tex; mode=display">
p\left( { { w }_{ i } }|{ { y }_{ k } } \right) =\frac { \sum _{ t=1 }^{ n }{ I\left\{ y_{ k }={ y }_{ t } \right\} x_{ i }^{ t } } +1 }
{ \sum _{ j=1 }^{ d }{ \sum _{ t=1 }^{ n }{ I\left\{ y_{ k }={ y }_{ t } \right\} x_{ i }^{ t } }  }  +d} 
</script>

<p>注意事项：</p>

<ul>
  <li>
    <p>训练时候对于词语平铺的文本,应该要做词的聚合,即行程 bag-of-words 的形式比较有利于后续统计计算,特别是对于伯努利 NB 必须做去重。当然,对于
多项式 NB,也可以顺次扫描累加。</p>
  </li>
  <li>
    <p>预测时候的概率连乘,为了防止精度损失,可以改用取 log 相加。</p>
  </li>
  <li>
    <p>对于短一些的文本,伯努利 NB 即可;对于长文本,考虑词频的多项式 NB 即
可。当然也可以使用 tf-idf 等特征值,仿照多项式 NB 的形式。</p>
  </li>
  <li>
    <p>预测时候,对于词表中出现但是本文章未出现的词语,伯努利 NB 下对得分有
贡献,多项式 NB 下不用考虑;对于在词表中未出现的词,都可以不予以考虑,
因为未登录词对各个类别的贡献是一样的。</p>
  </li>
</ul>

<h4 id="section">实验</h4>

<p>关于实验数据，可以到<a href="https://github.com/aluenkinglee/mlclass/tree/master/NativeBayes">这里</a>下载，训练集和测试集都已经很明白
总量在4000-的水平，result.dat是训练结果，可以看到测试集在训练数据的结果上准确率达到了100%。。这个是因为类别太少的缘故。只有3个类别，不过这个已经
可以看到朴素贝叶斯在工业界的应用可以达到较好的性能。</p>

<p>朴素贝叶斯分类器代码：</p>

<p>```java
/**
     * 训练目录下的样本集合
     * 
     * @throws IOException
     */
    public static void trainSamples() throws IOException {
        NaiveBayes classifier = new NaiveBayes();
        File flist = new File(“./data-trainning-set”);
        for (File f : flist.listFiles()) {
            classifier.training(new Instance(f));
        }
        classifier.save(new File(“result.dat”));
        System.out.println(“Trainning finished”);
    }</p>

<pre><code>/**
 * specify the training dataset directory, and store result in the outfile
 * 
 * @param directory
 * @param result
 * @throws IOException
 */
public static void trainSamples(String directory, String outfile)
        throws IOException {
    NaiveBayes classifier = new NaiveBayes();
    File flist = new File(directory);
    for (File f : flist.listFiles()) {
        classifier.training(new Instance(f));
    }
    classifier.save(new File(outfile));
    System.out.println("Trainning finished");
}

/**
 * 判断该实例所属的类别category
 * 
 * @param doc
 * @return
 */
public String getCategory(Instance doc) {
    Collection&lt;String&gt; categories = VARIABLE.getCategories();
    System.out.println(categories);
    double best = Double.NEGATIVE_INFINITY;
    String bestName = null;
    for (String c : categories) {
        double current = getProbability(c, doc);
        System.out.println(c + ":" + current);
        if (best &lt; current) {
            best = current;
            bestName = c;
        }
    }
    return bestName;
}

/**
 * 计算P（C)=该类型文档总数/文档总数，返回的数对数值
 * 
 * @param category
 * @return
 */
public double getCategoryProbability(String category) {
    return Math.log(VARIABLE.getDocCount(category) * 1.0f
            / VARIABLE.getDocCount());
}

/**
 * 计算P(feature|cateogry),返回的是取对数后的数值
 * 
 * @param feature
 * @param category
 * @return
 */
public double getFeatureProbability(String feature, String category) {
    int m = VARIABLE.getFeatureCount();
    return Math.log((VARIABLE.getDocCount(feature, category) + 1.0)
            / (VARIABLE.getDocCount(category) + m));
}

/**
 * 计算给定实例文档属于指定类别的概率，返回的是取对数后的数值
 * 
 * @param category
 * @param doc
 * @return
 */
public double getProbability(String category, Instance doc) {
    double result = getCategoryProbability(category);
    for (String feature : doc.getWords()) {
        if (VARIABLE.containFeature(feature)) {
            result += getFeatureProbability(feature, category);
        }
    }
    return result;
}

/**
 * 加载训练结果
 * 
 * @param file
 * @throws IOException
 */
public void load(File file) throws IOException {
    DataInputStream in = new DataInputStream(new FileInputStream(file));
    VARIABLE = Variable.read(in);
}

/**
 * 保存训练结果
 * 
 * @throws IOException
 */
void save(File file) throws IOException {
    DataOutput out = new DataOutputStream(new FileOutputStream(file));
    VARIABLE.write(out);
}

/**
 * 训练一篇文档
 * 
 * @param doc
 */
public void training(Instance doc) {
    VARIABLE.addInstance(doc);
} ```
</code></pre>

<p>特征词类代码：</p>

<p>```java</p>

<p>public class Feature {
    /** 每个关键词在不同类别中出现的文档数量 */
    private Map&lt;String, Integer&gt; docCountMap = new HashMap&lt;String, Integer&gt;();
    /** 特征名称 */
    private String name;</p>

<pre><code>public String getName() {
    return name;
}

public void setName(String name) {
    this.name = name;
}

public void incDocCount(String category) {
    if (docCountMap.containsKey(category)) {
        docCountMap.put(category, docCountMap.get(category) + 1);
    } else {
        docCountMap.put(category, 1);
    }
}

public int getDocCount(String category) {
    if (docCountMap.containsKey(category)) {
        return docCountMap.get(category);
    } else {
        return 0;
    }
}

public void write(DataOutput out) throws IOException {
    out.writeUTF(name == null ? "" : name);

    out.writeInt(docCountMap.size());
    for (String category : docCountMap.keySet()) {
        out.writeUTF(category);
        out.writeInt(docCountMap.get(category));
    }
}

public void readFields(DataInput in) throws IOException {
    this.name = in.readUTF();

    docCountMap = new HashMap&lt;String, Integer&gt;();
    int size = in.readInt();
    for (int i = 0; i &lt; size; i++) {
        String category = in.readUTF();
        int docCount = in.readInt();
        docCountMap.put(category, docCount);
    }
}

public static Feature read(DataInput in) throws IOException {
    Feature f = new Feature();
    f.readFields(in);
    return f;
}

public static void main(String[] args) {
    // TODO Auto-generated method stub

}
</code></pre>

<p>}
```</p>

<p>更多代码请看<a href="https://github.com/aluenkinglee/mlclass/tree/master/NativeBayes/src">这里</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[拉格朗日对偶]]></title>
    <link href="http://aluenkinglee.com/blog/2014/06/03/lagrange-duality/"/>
    <updated>2014-06-03T21:41:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/06/03/lagrange-duality</id>
    <content type="html"><![CDATA[<p>在解决约束最优化的额问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。该方法主要应用的SVM和最大熵模型中。</p>

<p>先抛开上面的二次规划问题，先来看看存在等式约束的极值问题求法，比如下面的最优化问题：</p>

<script type="math/tex; mode=display">
\min _{ w }{ f\left( w \right)  } \\ s.t\quad h_{ i }\left( w \right) =0,\quad i=1,\cdots ,l
</script>

<p>目标函数是f(w)，下面是等式约束。通常解法是引入拉格朗日算子，这里使用<script type="math/tex">\beta</script>来表示算子，得到拉格朗日公式为</p>

<!-- more -->

<script type="math/tex; mode=display">
L \left( w,\beta  \right) =f\left( w \right) +\sum _{ i=1 }^{ l }{ \beta _{ i }h_{ i } } \left( w \right) 
</script>

<p>L是等式约束的个数。</p>

<p>然后分别对w和 <script type="math/tex">\beta</script>求偏导，使得偏导数等于0，然后解出w和 <script type="math/tex">\beta_i</script>。至于为什么引入拉格朗日算子可以求出极值，原因是f(w)
的dw变化方向受其他不等式的约束，dw的变化方向与f(w)的梯度垂直时才能获得极值，而且在极值处，f(w)的梯度与其他等式梯度的线性组合平行，因此他们之间存在线性关系。（参考《最优化与KKT条件》）
然后我们探讨有不等式约束的极值问题求法，问题如下：</p>

<script type="math/tex; mode=display">
\min _{ w }{ f\left( w \right)  } \\ 
s.t\quad h_{ i }\left( w \right) =0,\quad i=1,\cdots ,l\\ 
\quad \quad \quad g_{ i }\left( w \right) \le 0,\quad i=1,\cdots ,k
</script>

<p>我们定义一般化的拉格朗日公式</p>

<script type="math/tex; mode=display">
L\left( w,\beta  \right) =f\left( w \right) +\sum _{ i=1 }^{ k }{ \alpha _{ i }g_{ i } } \left( w \right) \sum _{ i=1 }^{ l }{ \beta _{ i }h_{ i } } \left( w \right) 
</script>

<p>这里的<script type="math/tex">\alpha _{ i } </script>和<script type="math/tex">\beta _{ i } </script> 都是拉格朗日算子。如果按这个公式求解，会出现问题，因为我们求解的是最小值，而这里的 <script type="math/tex">g_{ i } \left( w \right) </script>已经不是0了，我们可以将 <script type="math/tex">\alpha _{ i } </script>调整成很大的正值，来使最后的函数结果是负无穷。因此我们需要排除这种情况，我们定义下面的函数：</p>

<script type="math/tex; mode=display">
{ \theta  }_{p  }\left( w \right) =\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  } 
</script>

<p>这里的P代表primal。假设<script type="math/tex">g_{ i }\left( w \right)</script> &gt;0 或者<script type="math/tex">h_{ i }\left( w \right) \neq 0</script> ，那么我们总是可以调整 <script type="math/tex">\alpha_i</script>和<script type="math/tex">\beta_i</script> 来使得<script type="math/tex"> { \theta  }_{ p  }\left( w \right)</script>有最大值为正无穷。而只有g和h满足约束时，<script type="math/tex"> { \theta  }_{ p }\left( w \right)</script> 为f(w)。这个函数的精妙之处在于 <script type="math/tex">\alpha_i \ge 0</script>，而且求极大值。</p>

<p>因此,</p>

<script type="math/tex; mode=display">
{ \theta  }_{ p  }\left( w \right) =\begin{cases} f\left( w \right) ,\quad w满足原始问题约束 \\ +\infty ，其他\quad  \end{cases}
</script>

<p>所以如果考虑极小化问题</p>

<script type="math/tex; mode=display">
{ \min _{ w }{ { \theta  }_{p  }\left( w \right)  }  }={ \min _{ w }{ \max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  }  }  }
</script>

<p>它是原始问题的等价解，原始最优化问题转化成了拉格朗日函数的极小极大问题，这时候把原始问题的最优值记为：</p>

<script type="math/tex; mode=display">
{ p }^{ \ast  }=\min _{ w }{ { \theta  }_{ p }\left( w \right)  } 
</script>

<p>哎哟，看看我们的等价形式哦，首先有两个参数，其中还是一个不等式约束=。=,考虑下对偶吧，极小极大问题转化为等价的极大极小问题。</p>

<h4 id="section">对偶形式</h4>

<p>定义<script type="math/tex">{ \theta  }_{ D }\left( \alpha ,\beta  \right) =\min _{ w }{ L\left( w,\alpha ,\beta  \right)  } </script>，在考虑极大化<script type="math/tex">{ \theta  }_{ D }\left( \alpha ,\beta  \right) </script>,先把这两个参数看成固定值，求关于w的最小值，之后在求对偶的最大值即</p>

<script type="math/tex; mode=display">
\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ { \theta  }_{ D }\left( \alpha ,\beta  \right)  } =\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ \min _{ w }{ L\left( w,\alpha ,\beta  \right)  }  } 
</script>

<p>这个问题是原问题的对偶问题，相对于原问题只是更换了min和max的顺序，而一般更换顺序的结果是Max Min(X) &lt;= MinMax(X)。然而在这里两者相等。用<script type="math/tex">{ d }^{ \ast  }</script> 来表示对偶问题如下：</p>

<script type="math/tex; mode=display">
{ d }^{ \ast  }=\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ \min _{ w }{ L\left( w,\alpha ,\beta  \right)  }  } \le { \min _{ w }{ \max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  }  }  }={ p }^{ \ast  }
 </script>

<p>存在 <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script> 使得<script type="math/tex">{ w }^{ \ast  }</script>是原问题的解，<script type="math/tex">{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script>  是对偶问题的解。还有 <script type="math/tex">{ p }^{ \ast  }={ d}^{ \ast  } = L({ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  })</script> </p>

<p>另外，  <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script> 满足库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition），该条件如下：</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-06-03-lagrange-duality/kkt.png?raw=true 库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition）&quot;" alt="库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition）" /></p>

<p>所以如果 <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script>  满足了库恩-塔克条件，那么他们就是原问题和对偶问题的解。当<script type="math/tex">g_{i}(w^{ \ast  })=0</script> 时，w处于可行域的边界上，这时才是起作用的约束。而其他位于可行域内部（ <script type="math/tex">% &lt;![CDATA[
g_{i}(w^{ \ast  })<0 %]]&gt;</script> 的）点都是不起作用的约束，其 <script type="math/tex">{ \alpha }^{ \ast  }=0</script>。这个KKT双重补足条件会用来解释支持向量和SMO的收敛测试。</p>

<p>KKT的总体思想是将极值会在可行域边界上取得，也就是不等式为0或等式约束里取得，而最优下降方向一般是这些等式的线性组合，其中每个元素要么是不等式为0的约束，要么是等式约束。对于在可行域边界内的点，对最优解不起作用，因此前面的系数为0。</p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<ol>
  <li>
    <p>Andrew Ng的原始课件讲义</p>
  </li>
  <li>
    <p>统计学习方法</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从逻辑回归分类到SVM分类]]></title>
    <link href="http://aluenkinglee.com/blog/2014/06/03/my-understanding-about-svm/"/>
    <updated>2014-06-03T20:53:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/06/03/my-understanding-about-svm</id>
    <content type="html"><![CDATA[<p>上一章讲到了线性回归的一个典型算法，核心思想就是利用最小二乘法最为损失函数，不断使用梯度下降法（或者使用随机梯度下降法（stochastic gradient descent））来更新theta值。</p>

<p>后来降到了分类，逻辑回归只不过有个很好的性质就是值分布在0到1之间，正好可以利用到分类上。logistic回归就是要学习得到θ，使得正例的特征远大于 0，负例的特征远
小于 0，强调在全部训练实例上达到这个目标。为什么说逻辑回归是个线性模型？这是因为该模型是将特性的线性组合作为自变量，然后使用logistic函数（或者说是sigmoid函数）将自变量映射到（0-1）上，将值和概率结合之后从而应用到分类上。
函数表示形式：</p>

<script type="math/tex; mode=display">
h_{ \theta  }\left( x \right) =g\left( \theta ^{ T }x \right) =\frac { 1 }{ 1+{e  }^{ -\theta^{T}x } }
</script>

<!-- more -->

<p>其中 x 是 n 维特征向量，函数 g 就是 logistic 函数
从线性回归到了逻辑回归，从逻辑回归又到了分类，那么再来看看SVM这个有监督的分类学习算法。
在这里我们使用的y的取值记为1和-1，所以对logistic 回归中的做下替换，令logistic回归中的y=0和y=1变为y=-1,y=1。同时将θ替换成 w 和 b。
所以有<script type="math/tex">\Theta^{T}x=\theta_0x_0+\theta_1x_1+\cdots +\theta_nx_n</script>，现在使用b替换<script type="math/tex">\theta_0</script>,替换后的形式变为<script type="math/tex">w^{ T }x=w_{ 1 }x_{ 1 }+\cdots +w_{ n }x_{ n }+b</script>，这样做之后，我们的假设函数就变成了</p>

<script type="math/tex; mode=display">
h_{ b,w }\left( x \right) =g\left( w^{ T }x+b \right)
</script>

<p>和逻辑回归的形式很相像。</p>

<p>表达形式就引申到这里，在来谈下SVM的思想，当我们学习线性回归时，我们的想法就是使用最小二乘法拟合数据，而在分类问题中，我们的想法就是找到一条直线，使正负样本离这个线或者超平面尽可能的远。也就是<code>间隔</code>最大。用一句话来说就是：<strong>在特征空间上的间隔最大的线性分类器。所以我们所有的努力都在如何是间隔最大化上，而这个可以转化为一个凸二次规划的问题</strong>。所以SVM的学习算法就是求解凸二次规划的最优化算法。</p>

<h5 id="functional-margingeometric-margin">函数间隔（functional margin）和几何间隔（geometric margin）</h5>

<p>我们定义函数间隔就是：对于给定的数据集T和超平面（w,b），样本点<script type="math/tex">\left( x^{(i)},y^{(i)} \right) </script>到超平面的函数间隔为：
$$
\widehat { \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b) 
$$</p>

<p>函数间隔或者间隔本身描述了一种确信度。离超平面越远，间隔值就越大，可信度就越大。</p>

<p>刚刚我们定义的函数间隔是针对某一个样本的，现在我们定义全局样本上的函数间隔，定义超平面关于数据集的函数间隔为超平面中所有样本点函数间隔的最小值，就是在训练样本上分类正例和负例确信度最小那个函数间隔，即</p>

<script type="math/tex; mode=display">
\widehat {\gamma }=\min _{ i=1,...m }{ \widehat { \gamma  } ^{ (i) } }
</script>

<p>但是有个问题，如果按比例的增加w和b，那么函数间隔也会按比例改变，这个对结果没有影响，但是问题会变得不好描述，不能定量的计算，所以我们就需要把它规范化(normalization)。只需要结果除以<script type="math/tex">\left\| w \right\| </script>就好了，这个时候<script type="math/tex">w/\left\| w \right\| </script>就成为了单位向量，所以函数间隔和几何间隔的关系也就是这样简单，几何间隔就是规范化后的函数间隔。无论w和b怎么折腾，几何间隔都不会改变。</p>

<p>定义几何间隔就是：对于给定的数据集T和超平面（w,b），样本点<script type="math/tex">\left( x^{(i)},y^{(i)} \right) </script>到超平面的几何间隔为：</p>

<script type="math/tex; mode=display">
{ \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b)/\left\| w \right\|
</script>

<p>定义超平面关于数据集的几何间隔为超平面中所有样本点几何间隔的最小值,即</p>

<script type="math/tex; mode=display">
{ \gamma  }=\min _{ i=1,...m }{ { \gamma  }^{ (i) } }
</script>

<p>最优间隔分类器（optimal margin classifier）（利用间隔最大化）</p>

<p>回想前面我们提到我们的目标是寻找一个超平面，使得离超平面比较近的点能有更大的
间距。 也就是我们不考虑所有的点都必须远离超平面，我们关心求得的超平面能够让所有点中离它最近的点具有最大间距。形象的说，我们将上面的图看作是一张纸，我们要找一条折线，按照这条折线折叠后，离折线最近的点的间距比其他折线都要大。形式化表示为：</p>

<script type="math/tex; mode=display">
\max _{ \gamma ,w,b }{ \gamma  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \gamma ,i=1,\cdots ,m\\ \left\| w \right\| =1
</script>

<table>
  <tbody>
    <tr>
      <td>这里用</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>=1 规约 w，使得<script type="math/tex"> w^{T}\cdot x+b</script>是几何间隔。</td>
    </tr>
  </tbody>
</table>

<p>到此，我们已经将模型定义出来了。如果求得了 w 和 b，那么来一个特征 x，我们就能
够分类了，称为最优间隔分类器。接下的问题就是如何求解 w 和 b 的问题了。</p>

<p>由于||w|| = 1不是凸函数，我们想先处理转化一下，考虑几何间隔和函数间隔的关系，
<script type="math/tex"> \gamma =\frac { \hat { \gamma  }  }{ \left\| w \right\|  } </script>，我们改写一下上面的式子：</p>

<script type="math/tex; mode=display">
\max _{ \gamma ,w,b }{ \frac { \widehat { \gamma  }  }{ \left\| w \right\|  }  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \widehat { \gamma  } ,i=1,\cdots ,m
</script>

<p>因为函数间隔值得改变对结果没有影响，所以可以给它个固定值比如1.将 <script type="math/tex">\hat { \gamma  } =1</script>代入上面的最优化问题，因为最大化<script type="math/tex"> \frac { 1 }{ \left\| w \right\|  } </script>最小化<script type="math/tex"> \frac { 1 }{ 2 } \left\| w \right\| ^{ 2 }</script>是等价的，于是将上面改写成这样：</p>

<script type="math/tex; mode=display">
\min _{ \gamma ,w,b }{ \frac { 1 }{ 2 }  } { \left\| w \right\|  }^{ 2 }\\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) -1\ge 0,i=1,\cdots ,m
</script>

<p>这就变成了一个凸二次规划问题，详情见<a href="http://zh.wikipedia.org/wiki/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92">凸二次规划</a></p>

<p>接下来是关于拉格朗日对偶的问题，在这之后，在描述SVM中最简单的分类器——<strong>线性可分支持向量机</strong>，因为这个情况下，数据是线性可分的，而且噪音没有，只需要通过<strong>硬间隔最大化</strong>,即可学习一个线性的分类器，又称硬间隔支持向量机。</p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<ol>
  <li>
    <p>Andrew Ng的原始课件讲义</p>
  </li>
  <li>
    <p>统计学习方法</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[generative method]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/30/generative-method/"/>
    <updated>2014-04-30T22:52:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/30/generative-method</id>
    <content type="html"><![CDATA[<p>线性回归模型和logistic回归是判别模型，也就是根据特征值来求结果的概率。形式化表
示为<script type="math/tex">p(y|x;\theta)</script>在参数<script type="math/tex">\theta</script>确定的情况下，求解条件概率<script type="math/tex">p(y|x)</script>。
通俗的解释为在给定特征后预测结果出现的概率。</p>

<p>就按照Andrew Ng讲的那样，确定肿瘤是良性的还是恶性的，可以使用判别模型的方法是先
从历史数据中学习到模型，然后通过提取肿瘤的特征来预测出它是良性恶性的概率。</p>

<p>反过来，要是我们先从良性肿瘤学习出良性肿瘤的模型，从恶性肿瘤学习出恶性肿瘤的模型，
然后提取肿瘤的特征，放到良性肿瘤的模型看下概率，在放到恶性肿瘤的模型看下概率，哪个
大是哪个。</p>

<p>形式化表示为求<script type="math/tex">P(X|Y)</script>,x是特征，y是类型即模型。
利用贝叶斯公式发现两个模型的统一性：</p>

<script type="math/tex; mode=display">
p(y|x)=\frac { p(x|y)p(y) }{ p(x) } 
</script>

<p>由于我们关注的是 y 的离散值结果中哪个概率大（比如良性肿瘤和恶性肿瘤哪个概率大），
而并不是关心具体的概率，因此上式改写为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\quad \quad \quad \quad \begin{eqnarray} \max _{ y }{ p(y|x) }  & = & \max _{ y }{ \frac { p(x|y)p(y) }{ p(x) }  }  \\  & = &\max _{ y } p(x|y)p(y) \end{eqnarray}
 %]]&gt;</script>

<table>
  <tbody>
    <tr>
      <td>其中$$p(x</td>
      <td>y)<script type="math/tex">称为后验概率,</script>p(y)$$称为先验概率.</td>
    </tr>
  </tbody>
</table>

<p>由<script type="math/tex">p(x|y)*p(y)=p(x,y)</script>,因此有时称判别模型求的是条件概率，生成模型求的是联
合概率。</p>

<p>常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件
随机场、神经网络等。</p>

<p>常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted 
Boltzmann Machine 等。</p>

<p>上篇博客较为详细地介绍了两个模型</p>

<h3 id="gaussian-discriminant-analysis">高斯判别分析（Gaussian discriminant analysis）</h3>

<h5 id="section">多维正太分布</h5>

<p>多变量正态分布描述的是n维随机变量的分布情况。所以这里的<script type="math/tex">\mu </script>变成了n维随机变量，<script type="math/tex">\sigma </script>也变成了
矩阵<script type="math/tex">\Sigma </script>.记做<script type="math/tex">N(\mu,\Sigma)</script>.假设有 n 个随机变量<script type="math/tex">X_1,X_2,\cdots ,X_n</script>.所以显而易见，<script type="math/tex">\mu </script>的第i个分量是<script type="math/tex">E(X_i),\Sigma_{ii}=Var(X_i),\Sigma_{ij}=Cov(X_i,X_j)</script>.</p>

<p>概率密度函数如下：</p>

<script type="math/tex; mode=display">
p(x;\mu,\Sigma)=\frac { 1 }{ \left( 2\pi  \right) ^{ n/2 }\left| \Sigma  \right| ^{ 1/2 } } exp\left( -\frac { 1 }{ 2 } \left( x-\mu \right)^T \Sigma^{-1}{\left(x-\mu\right)} \right) 
</script>

<h5 id="section-1">模型分析与应用</h5>

<table>
  <tbody>
    <tr>
      <td>如果输入特征x连续型随机变量，那么可以使用高斯判别分析模型来确定$$p(x</td>
      <td>y)$$。模型如下,先以二元分布即伯努利分布来说（因为前面的例子是二元的）:</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">
y\sim Bernoulli\left( \phi \right) \\
x|y=0\sim N(\mu_0,\Sigma)\\
x|y=1\sim N(\mu_1,\Sigma)
</script>

]]></content>
  </entry>
  
</feed>
