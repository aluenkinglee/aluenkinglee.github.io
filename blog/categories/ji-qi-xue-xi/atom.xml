<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | 曾经渐行远，未免心戚戚]]></title>
  <link href="http://aluenkinglee.com/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://aluenkinglee.com/"/>
  <updated>2014-06-04T09:24:35+08:00</updated>
  <id>http://aluenkinglee.com/</id>
  <author>
    <name><![CDATA[Aluen King Lee]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[拉格朗日对偶]]></title>
    <link href="http://aluenkinglee.com/blog/2014/06/03/lagrange-duality/"/>
    <updated>2014-06-03T21:41:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/06/03/lagrange-duality</id>
    <content type="html"><![CDATA[<p>在解决约束最优化的额问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。该方法主要应用的SVM和最大熵模型中。</p>

<p>先抛开上面的二次规划问题，先来看看存在等式约束的极值问题求法，比如下面的最优化问题：</p>

<script type="math/tex; mode=display">
\min _{ w }{ f\left( w \right)  } \\ s.t\quad h_{ i }\left( w \right) =0,\quad i=1,\cdots ,l
</script>

<p>目标函数是f(w)，下面是等式约束。通常解法是引入拉格朗日算子，这里使用<script type="math/tex">\beta</script>来表示算子，得到拉格朗日公式为</p>

<!-- more -->

<script type="math/tex; mode=display">
L \left( w,\beta  \right) =f\left( w \right) +\sum _{ i=1 }^{ l }{ \beta _{ i }h_{ i } } \left( w \right) 
</script>

<p>L是等式约束的个数。</p>

<p>然后分别对w和 <script type="math/tex">\beta</script>求偏导，使得偏导数等于0，然后解出w和 <script type="math/tex">\beta_i</script>。至于为什么引入拉格朗日算子可以求出极值，原因是f(w)
的dw变化方向受其他不等式的约束，dw的变化方向与f(w)的梯度垂直时才能获得极值，而且在极值处，f(w)的梯度与其他等式梯度的线性组合平行，因此他们之间存在线性关系。（参考《最优化与KKT条件》）
然后我们探讨有不等式约束的极值问题求法，问题如下：</p>

<script type="math/tex; mode=display">
\min _{ w }{ f\left( w \right)  } \\ 
s.t\quad h_{ i }\left( w \right) =0,\quad i=1,\cdots ,l\\ 
\quad \quad \quad g_{ i }\left( w \right) \le 0,\quad i=1,\cdots ,k
</script>

<p>我们定义一般化的拉格朗日公式</p>

<script type="math/tex; mode=display">
L\left( w,\beta  \right) =f\left( w \right) +\sum _{ i=1 }^{ k }{ \alpha _{ i }g_{ i } } \left( w \right) \sum _{ i=1 }^{ l }{ \beta _{ i }h_{ i } } \left( w \right) 
</script>

<p>这里的<script type="math/tex">\alpha _{ i } </script>和<script type="math/tex">\beta _{ i } </script> 都是拉格朗日算子。如果按这个公式求解，会出现问题，因为我们求解的是最小值，而这里的 <script type="math/tex">g_{ i } \left( w \right) </script>已经不是0了，我们可以将 <script type="math/tex">\alpha _{ i } </script>调整成很大的正值，来使最后的函数结果是负无穷。因此我们需要排除这种情况，我们定义下面的函数：</p>

<script type="math/tex; mode=display">
{ \theta  }_{p  }\left( w \right) =\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  } 
</script>

<p>这里的P代表primal。假设<script type="math/tex">g_{ i }\left( w \right)</script> &gt;0 或者<script type="math/tex">h_{ i }\left( w \right) \neq 0</script> ，那么我们总是可以调整 <script type="math/tex">\alpha_i</script>和<script type="math/tex">\beta_i</script> 来使得<script type="math/tex"> { \theta  }_{ p  }\left( w \right)</script>有最大值为正无穷。而只有g和h满足约束时，<script type="math/tex"> { \theta  }_{ p }\left( w \right)</script> 为f(w)。这个函数的精妙之处在于 <script type="math/tex">\alpha_i \ge 0</script>，而且求极大值。</p>

<p>因此,</p>

<script type="math/tex; mode=display">
{ \theta  }_{ p  }\left( w \right) =\begin{cases} f\left( w \right) ,\quad w满足原始问题约束 \\ +\infty ，其他\quad  \end{cases}
</script>

<p>所以如果考虑极小化问题</p>

<script type="math/tex; mode=display">
{ \min _{ w }{ { \theta  }_{p  }\left( w \right)  }  }={ \min _{ w }{ \max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  }  }  }
</script>

<p>它是原始问题的等价解，原始最优化问题转化成了拉格朗日函数的极小极大问题，这时候把原始问题的最优值记为：</p>

<script type="math/tex; mode=display">
{ p }^{ \ast  }=\min _{ w }{ { \theta  }_{ p }\left( w \right)  } 
</script>

<p>哎哟，看看我们的等价形式哦，首先有两个参数，其中还是一个不等式约束=。=,考虑下对偶吧，极小极大问题转化为等价的极大极小问题。</p>

<h4 id="section">对偶形式</h4>

<p>定义<script type="math/tex">{ \theta  }_{ D }\left( \alpha ,\beta  \right) =\min _{ w }{ L\left( w,\alpha ,\beta  \right)  } </script>，在考虑极大化<script type="math/tex">{ \theta  }_{ D }\left( \alpha ,\beta  \right) </script>,先把这两个参数看成固定值，求关于w的最小值，之后在求对偶的最大值即</p>

<script type="math/tex; mode=display">
\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ { \theta  }_{ D }\left( \alpha ,\beta  \right)  } =\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ \min _{ w }{ L\left( w,\alpha ,\beta  \right)  }  } 
</script>

<p>这个问题是原问题的对偶问题，相对于原问题只是更换了min和max的顺序，而一般更换顺序的结果是Max Min(X) &lt;= MinMax(X)。然而在这里两者相等。用<script type="math/tex">{ d }^{ \ast  }</script> 来表示对偶问题如下：</p>

<script type="math/tex; mode=display">
{ d }^{ \ast  }=\max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ \min _{ w }{ L\left( w,\alpha ,\beta  \right)  }  } \le { \min _{ w }{ \max _{ \alpha ,\beta :{ \alpha  }_{ i }\ge 0 }{ L\left( w,\alpha ,\beta  \right)  }  }  }={ p }^{ \ast  }
 </script>

<p>存在 <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script> 使得<script type="math/tex">{ w }^{ \ast  }</script>是原问题的解，<script type="math/tex">{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script>  是对偶问题的解。还有 <script type="math/tex">{ p }^{ \ast  }={ d}^{ \ast  } = L({ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  })</script> </p>

<p>另外，  <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script> 满足库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition），该条件如下：</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-06-03-lagrange-duality/kkt.png?raw=true 库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition）&quot;" alt="库恩-塔克条件（Karush-Kuhn-Tucker, KKT condition）" /></p>

<p>所以如果 <script type="math/tex">{ w }^{ \ast  },{ \alpha }^{ \ast  },{ \beta }^{ \ast  }</script>  满足了库恩-塔克条件，那么他们就是原问题和对偶问题的解。当<script type="math/tex">g_{i}(w^{ \ast  })=0</script> 时，w处于可行域的边界上，这时才是起作用的约束。而其他位于可行域内部（ <script type="math/tex">% &lt;![CDATA[
g_{i}(w^{ \ast  })<0 %]]&gt;</script> 的）点都是不起作用的约束，其 <script type="math/tex">{ \alpha }^{ \ast  }=0</script>。这个KKT双重补足条件会用来解释支持向量和SMO的收敛测试。</p>

<p>KKT的总体思想是将极值会在可行域边界上取得，也就是不等式为0或等式约束里取得，而最优下降方向一般是这些等式的线性组合，其中每个元素要么是不等式为0的约束，要么是等式约束。对于在可行域边界内的点，对最优解不起作用，因此前面的系数为0。</p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<ol>
  <li>
    <p>Andrew Ng的原始课件讲义</p>
  </li>
  <li>
    <p>统计学习方法</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从逻辑回归分类到SVM分类]]></title>
    <link href="http://aluenkinglee.com/blog/2014/06/03/my-understanding-about-svm/"/>
    <updated>2014-06-03T20:53:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/06/03/my-understanding-about-svm</id>
    <content type="html"><![CDATA[<p>上一章讲到了线性回归的一个典型算法，核心思想就是利用最小二乘法最为损失函数，不断使用梯度下降法（或者使用随机梯度下降法（stochastic gradient descent））来更新theta值。</p>

<p>后来降到了分类，逻辑回归只不过有个很好的性质就是值分布在0到1之间，正好可以利用到分类上。logistic回归就是要学习得到θ，使得正例的特征远大于 0，负例的特征远
小于 0，强调在全部训练实例上达到这个目标。为什么说逻辑回归是个线性模型？这是因为该模型是将特性的线性组合作为自变量，然后使用logistic函数（或者说是sigmoid函数）将自变量映射到（0-1）上，将值和概率结合之后从而应用到分类上。
函数表示形式：</p>

<script type="math/tex; mode=display">
h_{ \theta  }\left( x \right) =g\left( \theta ^{ T }x \right) =\frac { 1 }{ 1+{e  }^{ -\theta^{T}x } }
</script>

<!-- more -->

<p>其中 x 是 n 维特征向量，函数 g 就是 logistic 函数
从线性回归到了逻辑回归，从逻辑回归又到了分类，那么再来看看SVM这个有监督的分类学习算法。
在这里我们使用的y的取值记为1和-1，所以对logistic 回归中的做下替换，令logistic回归中的y=0和y=1变为y=-1,y=1。同时将θ替换成 w 和 b。
所以有<script type="math/tex">\Theta^{T}x=\theta_0x_0+\theta_1x_1+\cdots +\theta_nx_n</script>，现在使用b替换<script type="math/tex">\theta_0</script>,替换后的形式变为<script type="math/tex">w^{ T }x=w_{ 1 }x_{ 1 }+\cdots +w_{ n }x_{ n }+b</script>，这样做之后，我们的假设函数就变成了</p>

<script type="math/tex; mode=display">
h_{ b,w }\left( x \right) =g\left( w^{ T }x+b \right)
</script>

<p>和逻辑回归的形式很相像。</p>

<p>表达形式就引申到这里，在来谈下SVM的思想，当我们学习线性回归时，我们的想法就是使用最小二乘法拟合数据，而在分类问题中，我们的想法就是找到一条直线，使正负样本离这个线或者超平面尽可能的远。也就是<code>间隔</code>最大。用一句话来说就是：<strong>在特征空间上的间隔最大的线性分类器。所以我们所有的努力都在如何是间隔最大化上，而这个可以转化为一个凸二次规划的问题</strong>。所以SVM的学习算法就是求解凸二次规划的最优化算法。</p>

<h5 id="functional-margingeometric-margin">函数间隔（functional margin）和几何间隔（geometric margin）</h5>

<p>我们定义函数间隔就是：对于给定的数据集T和超平面（w,b），样本点<script type="math/tex">\left( x^{(i)},y^{(i)} \right) </script>到超平面的函数间隔为：
$$
\widehat { \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b) 
$$</p>

<p>函数间隔或者间隔本身描述了一种确信度。离超平面越远，间隔值就越大，可信度就越大。</p>

<p>刚刚我们定义的函数间隔是针对某一个样本的，现在我们定义全局样本上的函数间隔，定义超平面关于数据集的函数间隔为超平面中所有样本点函数间隔的最小值，就是在训练样本上分类正例和负例确信度最小那个函数间隔，即</p>

<script type="math/tex; mode=display">
\widehat {\gamma }=\min _{ i=1,...m }{ \widehat { \gamma  } ^{ (i) } }
</script>

<p>但是有个问题，如果按比例的增加w和b，那么函数间隔也会按比例改变，这个对结果没有影响，但是问题会变得不好描述，不能定量的计算，所以我们就需要把它规范化(normalization)。只需要结果除以<script type="math/tex">\left\| w \right\| </script>就好了，这个时候<script type="math/tex">w/\left\| w \right\| </script>就成为了单位向量，所以函数间隔和几何间隔的关系也就是这样简单，几何间隔就是规范化后的函数间隔。无论w和b怎么折腾，几何间隔都不会改变。</p>

<p>定义几何间隔就是：对于给定的数据集T和超平面（w,b），样本点<script type="math/tex">\left( x^{(i)},y^{(i)} \right) </script>到超平面的几何间隔为：</p>

<script type="math/tex; mode=display">
{ \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b)/\left\| w \right\|
</script>

<p>定义超平面关于数据集的几何间隔为超平面中所有样本点几何间隔的最小值,即</p>

<script type="math/tex; mode=display">
{ \gamma  }=\min _{ i=1,...m }{ { \gamma  }^{ (i) } }
</script>

<p>最优间隔分类器（optimal margin classifier）（利用间隔最大化）</p>

<p>回想前面我们提到我们的目标是寻找一个超平面，使得离超平面比较近的点能有更大的
间距。 也就是我们不考虑所有的点都必须远离超平面，我们关心求得的超平面能够让所有点中离它最近的点具有最大间距。形象的说，我们将上面的图看作是一张纸，我们要找一条折线，按照这条折线折叠后，离折线最近的点的间距比其他折线都要大。形式化表示为：</p>

<script type="math/tex; mode=display">
\max _{ \gamma ,w,b }{ \gamma  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \gamma ,i=1,\cdots ,m\\ \left\| w \right\| =1
</script>

<table>
  <tbody>
    <tr>
      <td>这里用</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>=1 规约 w，使得<script type="math/tex"> w^{T}\cdot x+b</script>是几何间隔。</td>
    </tr>
  </tbody>
</table>

<p>到此，我们已经将模型定义出来了。如果求得了 w 和 b，那么来一个特征 x，我们就能
够分类了，称为最优间隔分类器。接下的问题就是如何求解 w 和 b 的问题了。</p>

<p>由于||w|| = 1不是凸函数，我们想先处理转化一下，考虑几何间隔和函数间隔的关系，
<script type="math/tex"> \gamma =\frac { \hat { \gamma  }  }{ \left\| w \right\|  } </script>，我们改写一下上面的式子：</p>

<script type="math/tex; mode=display">
\max _{ \gamma ,w,b }{ \frac { \widehat { \gamma  }  }{ \left\| w \right\|  }  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \widehat { \gamma  } ,i=1,\cdots ,m
</script>

<p>因为函数间隔值得改变对结果没有影响，所以可以给它个固定值比如1.将 <script type="math/tex">\hat { \gamma  } =1</script>代入上面的最优化问题，因为最大化<script type="math/tex"> \frac { 1 }{ \left\| w \right\|  } </script>最小化<script type="math/tex"> \frac { 1 }{ 2 } \left\| w \right\| ^{ 2 }</script>是等价的，于是将上面改写成这样：</p>

<script type="math/tex; mode=display">
\min _{ \gamma ,w,b }{ \frac { 1 }{ 2 }  } { \left\| w \right\|  }^{ 2 }\\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) -1\ge 0,i=1,\cdots ,m
</script>

<p>这就变成了一个凸二次规划问题，详情见<a href="http://zh.wikipedia.org/wiki/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92">凸二次规划</a></p>

<p>接下来是关于拉格朗日对偶的问题，在这之后，在描述SVM中最简单的分类器——<strong>线性可分支持向量机</strong>，因为这个情况下，数据是线性可分的，而且噪音没有，只需要通过<strong>硬间隔最大化</strong>,即可学习一个线性的分类器，又称硬间隔支持向量机。</p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<ol>
  <li>
    <p>Andrew Ng的原始课件讲义</p>
  </li>
  <li>
    <p>统计学习方法</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[generative method]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/30/generative-method/"/>
    <updated>2014-04-30T22:52:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/30/generative-method</id>
    <content type="html"><![CDATA[<p>线性回归模型和logistic回归是判别模型，也就是根据特征值来求结果的概率。形式化表
示为<script type="math/tex">p(y|x;\theta)</script>在参数<script type="math/tex">\theta</script>确定的情况下，求解条件概率<script type="math/tex">p(y|x)</script>。
通俗的解释为在给定特征后预测结果出现的概率。</p>

<p>就按照Andrew Ng讲的那样，确定肿瘤是良性的还是恶性的，可以使用判别模型的方法是先
从历史数据中学习到模型，然后通过提取肿瘤的特征来预测出它是良性恶性的概率。</p>

<p>反过来，要是我们先从良性肿瘤学习出良性肿瘤的模型，从恶性肿瘤学习出恶性肿瘤的模型，
然后提取肿瘤的特征，放到良性肿瘤的模型看下概率，在放到恶性肿瘤的模型看下概率，哪个
大是哪个。</p>

<p>形式化表示为求<script type="math/tex">P(X|Y)</script>,x是特征，y是类型即模型。
利用贝叶斯公式发现两个模型的统一性：</p>

<script type="math/tex; mode=display">
p(y|x)=\frac { p(x|y)p(y) }{ p(x) } 
</script>

<p>由于我们关注的是 y 的离散值结果中哪个概率大（比如良性肿瘤和恶性肿瘤哪个概率大），
而并不是关心具体的概率，因此上式改写为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\quad \quad \quad \quad \begin{eqnarray} \max _{ y }{ p(y|x) }  & = & \max _{ y }{ \frac { p(x|y)p(y) }{ p(x) }  }  \\  & = &\max _{ y } p(x|y)p(y) \end{eqnarray}
 %]]&gt;</script>

<table>
  <tbody>
    <tr>
      <td>其中$$p(x</td>
      <td>y)<script type="math/tex">称为后验概率,</script>p(y)$$称为先验概率.</td>
    </tr>
  </tbody>
</table>

<p>由<script type="math/tex">p(x|y)*p(y)=p(x,y)</script>,因此有时称判别模型求的是条件概率，生成模型求的是联
合概率。</p>

<p>常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件
随机场、神经网络等。</p>

<p>常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted 
Boltzmann Machine 等。</p>

<p>上篇博客较为详细地介绍了两个模型</p>

<h3 id="gaussian-discriminant-analysis">高斯判别分析（Gaussian discriminant analysis）</h3>

<h5 id="section">多维正太分布</h5>

<p>多变量正态分布描述的是n维随机变量的分布情况。所以这里的<script type="math/tex">\mu </script>变成了n维随机变量，<script type="math/tex">\sigma </script>也变成了
矩阵<script type="math/tex">\Sigma </script>.记做<script type="math/tex">N(\mu,\Sigma)</script>.假设有 n 个随机变量<script type="math/tex">X_1,X_2,\cdots ,X_n</script>.所以显而易见，<script type="math/tex">\mu </script>的第i个分量是<script type="math/tex">E(X_i),\Sigma_{ii}=Var(X_i),\Sigma_{ij}=Cov(X_i,X_j)</script>.</p>

<p>概率密度函数如下：</p>

<script type="math/tex; mode=display">
p(x;\mu,\Sigma)=\frac { 1 }{ \left( 2\pi  \right) ^{ n/2 }\left| \Sigma  \right| ^{ 1/2 } } exp\left( -\frac { 1 }{ 2 } \left( x-\mu \right)^T \Sigma^{-1}{\left(x-\mu\right)} \right) 
</script>

<h5 id="section-1">模型分析与应用</h5>

<table>
  <tbody>
    <tr>
      <td>如果输入特征x连续型随机变量，那么可以使用高斯判别分析模型来确定$$p(x</td>
      <td>y)$$。模型如下,先以二元分布即伯努利分布来说（因为前面的例子是二元的）:</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">
y\sim Bernoulli\left( \phi \right) \\
x|y=0\sim N(\mu_0,\Sigma)\\
x|y=1\sim N(\mu_1,\Sigma)
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Discriminative Model Vs. Generative Model]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/30/discriminative-model-vs-generative-model/"/>
    <updated>2014-04-30T17:17:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/30/discriminative-model-vs-generative-model</id>
    <content type="html"><![CDATA[<h2 id="section">判别模型和生成模型分析</h2>

<p>在学习复习ML内容时，中文检索生成模型搜到了该<a href="http://blog.sciencenet.cn/home.php?mod=space&amp;uid=248173&amp;do=blog&amp;id=227964">这里</a>本文主要参考该文章，并稍作整理。</p>

<!-- more -->

<p>这两者进行预测的方式不同在于模型的处理上：</p>

<p><strong>生成模型</strong>：无穷样本 ==&gt; 概率密度模型 = 产生模型 ==&gt; 预测</p>

<p><strong>判别模型</strong>：有限样本 ==&gt; 判别函数 = 预测模型 ==&gt; 预测</p>

<p>简单的说，假设<script type="math/tex">x</script>是观察值，<script type="math/tex">y</script>是模型。</p>

<p>如果对先验概率<script type="math/tex">P(x|y)</script>建模，就是<strong>生成模型（Generative modle）</strong>。
其基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大。
这种方法一般建立在统计力学和bayes理论的基础之上。</p>

<p>如果对条件概率(后验概率)<script type="math/tex">P(y|x)</script>建模，就是<strong>判别模型（Discrminative modle）</strong>。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。代表性理论为统计学习理论。
这两种方法目前交叉较多。</p>

<h3 id="discriminative-model">判别模型Discriminative Model</h3>

<p>又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)，即<script type="math/tex"> p(class|context)</script>。按照上文的记法就是<script type="math/tex">P(y|x)</script>
利用正负例和分类标签，焦点在判别模型的边缘分布。目标函数直接对应于分类准确率。</p>

<h6 id="section-1">主要特点：</h6>

<p>寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p>

<h6 id="section-2">优点:</h6>

<ol>
  <li>
    <p>分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。</p>
  </li>
  <li>
    <p>能清晰的分辨出多类或某一类与其他类之间的差异特征</p>
  </li>
  <li>
    <p>在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好</p>
  </li>
  <li>
    <p>适用于较多类别的识别</p>
  </li>
  <li>
    <p>判别模型的性能比生成模型要简单，比较容易学习</p>
  </li>
</ol>

<h6 id="section-3">缺点:</h6>

<ol>
  <li>
    <p>不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。</p>
  </li>
  <li>
    <p>Lack elegance of generative: Priors, 结构, 不确定性</p>
  </li>
  <li>
    <p>Alternative notions of penalty functions, regularization, 核函数</p>
  </li>
  <li>
    <p>黑盒操作: 变量间的关系不清楚，不可视</p>
  </li>
</ol>

<p><strong>常见的机器学习方法</strong>：</p>

<ol>
  <li>
    <p>logistic regression</p>
  </li>
  <li>
    <p>支持向量机（SVM）</p>
  </li>
  <li>
    <p>传统的神经网络（traditional neural networks）</p>
  </li>
  <li>
    <p>K近邻，最近邻（Nearest neighbor）</p>
  </li>
  <li>
    <p>Conditional random fields(CRF): 目前最新提出的热门模型，从NLP领域产生的，正在向ASR和CV上发展。</p>
  </li>
</ol>

<h6 id="section-4">主要应用：</h6>

<ol>
  <li>
    <p>图像文本分类</p>
  </li>
  <li>
    <p>生物科学分析</p>
  </li>
  <li>
    <p>时间序列预测</p>
  </li>
</ol>

<h3 id="generative-model">生成模型Generative Model</h3>

<table>
  <tbody>
    <tr>
      <td>估计的是联合概率分布（joint probability distribution），$$p(class, context)=p(class</td>
      <td>context)*p(context)<script type="math/tex">,换用之前的描述就是</script>p(y, x)=p(y</td>
      <td>x)*p(x)$$.</td>
    </tr>
  </tbody>
</table>

<p>用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的draw建模），或作为生成条件概率密度函数的中间步骤。通过使用<strong>贝叶斯定律</strong>可以从生成模型中得到条件分布。</p>

<p>如果观察到的数据是完全由生成模型所生成的，那么就可以拟合生成模型的参数，从而仅可能的增加数据相似度。但观测数据往往完全从生成模型得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。</p>

<h6 id="section-5">主要特点:</h6>

<ol>
  <li>
    <p>一般主要是对<strong>后验概率</strong>建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度</p>
  </li>
  <li>
    <p>只关注自己的inclass本身（即点左下角区域内的概率），不关心到底判别边界在哪</p>
  </li>
</ol>

<h6 id="section-6">优点:</h6>

<ol>
  <li>
    <p>模型可以通过增量学习得到（同意！）</p>
  </li>
  <li>
    <p>研究单类问题比判别模型灵活性强(怀疑？)</p>
  </li>
  <li>
    <p>能用于数据不完整（missing data）情况(怀疑？)</p>
  </li>
  <li>
    <p>实际上带的信息要比判别模型丰富（多太多！同意！）</p>
  </li>
  <li>
    <p>prior knowledge can be easily taken into account（同意！）</p>
  </li>
  <li>
    <p>modular construction of composed solutions to complex problems</p>
  </li>
  <li>
    <p>robust to partial occlusion and viewpoint changes</p>
  </li>
  <li>
    <p>can tolerate significant intra-class variation of object appearance</p>
  </li>
</ol>

<h6 id="section-7">缺点:</h6>

<ol>
  <li>学习和计算过程比较复杂</li>
</ol>

<p><strong>常见的机器学习方法</strong>：</p>

<ol>
  <li>
    <p><strong>Gaussians判别分析</strong></p>
  </li>
  <li>
    <p>** Naive Bayes**， Bayesian networks</p>
  </li>
  <li>
    <p>Mixtures of Gaussians（混合高斯模型）</p>
  </li>
  <li>
    <p>HMMs，Markov random fields</p>
  </li>
  <li>
    <p>Sigmoidal belief networks</p>
  </li>
</ol>

<h4 id="section-8">两者之间的关系</h4>

<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>

<blockquote>
  <blockquote>
    <p>参考</p>
  </blockquote>
</blockquote>

<p>http://prfans.com/forum/viewthread.php?tid=80</p>

<p>http://hi.baidu.com/cat_ng/blog/item/5e59c3cea730270593457e1d.html</p>

<p>http://en.wikipedia.org/wiki/Generative_model</p>

<p>http://blog.csdn.net/yangleecool/archive/2009/04/05/4051029.aspx</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[a scratch of feature selection in traffic classification]]></title>
    <link href="http://aluenkinglee.com/blog/2014/04/20/a-scratch-of-feature-selection-in-traffic-classification/"/>
    <updated>2014-04-20T21:02:00+08:00</updated>
    <id>http://aluenkinglee.com/blog/2014/04/20/a-scratch-of-feature-selection-in-traffic-classification</id>
    <content type="html"><![CDATA[<p>特征选取在减轻识别流量监测方面起着很重要的作用。该方法可以显著提高计算流量分类的性能。但是，大部分的特征不能应用在实时在线的流量分类中（有些特征只能在获取完整个流量才能得到，比如传输的数据大小，流的传输时长等）。所以在抉择分类的时候，需要一个优化过的特征集合在更短的时间内完成流量的分类。另外一种方案就是使用新型的网络架构如SDN/OpenFlow在目前已有的特征选择方法中，Chi-squared, Fuzzy-rough and Consistency-based的特征选择方法最适合P2P流量选择（那现在的手机端流量分析怎么样？）这些算法在使用ML进行在线P2P检测时会给出较好的特征子集。
<!--more-->
特征选取是寻找一个最小特征子集，可以快速有效的识别出实例的类别。如果利用一个特征进行分类聚类的结果与不使用它的结果没有很大的差别，则称整个特征时没有分类能力的。使用这些具备分类，聚类能力的特征，在分类的准确性和计算性能上都会得到提升。[1]主要研究的是在线流量分类中的流特征问题。然后考虑精度和性能的因素，选取了3中能够应用到P2P流量中的特征选择方法。</p>

<p>[1]实现的主要方法是使用了几个特征选择算法来提出在线的流量特征，使用J48算法作为分类器。</p>

<p>特征规模大小与分类器的效率和准确率息息相关，最优的特征集合可以减少分类器的建模和检测时间，从而提升分类器的性能[5]。主流的分类器有CSF, CON, Filter-Sub, Fuzzy-rough, Symmetrical-Uncert, Chi-squared,Info Gain, Relief, Principal and Latent-semantic。作者使用的Chi-squared, Consistency and fuzzy-rough算法，相关文献可以在论文[1]中找到。</p>

<h2 id="section">在线特征提取</h2>

<p>尽管Moore提出了248中流量特征，这些特征源自于同一个流中的报头信息。实际应用中的确不能全部都用到。具体操作是对现有的特征集使用那十个特征选择方法，分别选出各自的特征子集，然后应用到SVM分类器中，判别的准则为建模时间(训练时间)和准确率。然后合并准确率最高的前3个特征集合的并集作为最优特征子集。然后在分出那些特征可以在线获取（SOF-selection of features），他们作为分类器的输入-报文的特征向量。</p>

<p>核心思想是使用监督方法对有标记的数据集进行分类时，对特征集合进行规约，减少特征集的大小。</p>

<h6 id="section-1">使用信息熵增益算法对特征进行降纬</h6>

<p>实验前,<a href="http://www.aluenkinglee.com/blog/2014/04/21/feature-selection--infomation-gain/">信息熵增益算法</a></p>

<p><code>text
Instances:    42
Attributes:   35
              sip
              sport
              dip
              dport
              protocal
              interval1
              interval2
              interval3
              interval4
              interval5
              interval6
              interval7
              interval8
              interval9
              packet_len1
              packet_len2
              packet_len3
              packet_len4
              packet_len5
              packet_len6
              packet_len7
              packet_len8
              packet_len9
              packet_len10
              payload_len1
              payload_len2
              payload_len3
              payload_len4
              payload_len5
              payload_len6
              payload_len7
              payload_len8
              payload_len9
              payload_len10
              cluster
</code></p>

<p>得到的属性的排序是</p>

<p>```text
Ranked attributes:
 1.944968503161257472   12 interval7
 1.79590506127720192    22 packet_len8
 1.789000161744105216    9 interval4
 1.485875203840541952   32 payload_len8
 1.476115354039271936   24 packet_len10
 1.474554148784290048   19 packet_len5
 1.433523944277332992   11 interval6
 1.4172094615274624     20 packet_len6
 1.40428856745235968     6 interval1
 1.326924790992714496   34 payload_len10
 1.181689898847241984   18 packet_len4
 1.105237515513706624   23 packet_len9
 1.009802257207393664   28 payload_len4
 1.000000000000000896   31 payload_len7
 1.000000000000000896   30 payload_len6
 0.993447238380203776   21 packet_len7
 0.868563607479333888    3 dip
 0.832352013234566144   14 interval9
 0.829607103088203904   29 payload_len5
 0.781988055469156096   17 packet_len3
 0.781988055469156096   16 packet_len2
 0.737113917996471168   10 interval5
 0.544053730963280448    2 sport
 0                      27 payload_len3
 0                       5 protocal
 0                       1 sip
 0                       4 dport
 0                      33 payload_len9
 0                      25 payload_len1
 0                      15 packet_len1
 0                      13 interval8
 0                       7 interval2
 0                      26 payload_len2
 0                       8 interval3</p>

<p>```</p>

<p>所以对于属性payload_len3，protocal，sip，dport，payload_len9，payload_len1，payload_len2，packet_len1，interval8，interval2，interval3这十一个属性都可以去掉。
降维之后的属性集合大小为23.</p>

<p>降维之后的聚类结果</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-04-20-a-scratch-of-feature-selection-in-traffic-classification/reduction2.png?raw=true" alt="降维之后的聚类结果" title="降维之后的聚类结果" /></p>

<p>降维之前的聚类结果</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-04-20-a-scratch-of-feature-selection-in-traffic-classification/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%B7%E8%B7%9D%E7%A6%BB.png?raw=true" alt="降维之前的聚类结果" title="降维之前的聚类结果" /></p>

<p>效果非常吻合。</p>

<h6 id="pca">使用PCA对特征集规约</h6>

<p>这个使用weka的来做的，做出来之后有12个特征（都是原有特征的线性组合）</p>

<p><code>text
Attributes:   12
              0.333payload_len7+0.332packet_len7-0.307dip+0.295payload_len6+0.293packet_len6...
              0.399packet_len8+0.398payload_len8-0.269payload_len5-0.268packet_len5+0.261interval5...
              0.399interval4+0.309interval1-0.308packet_len10-0.274packet_len9-0.272payload_len9...
              -0.366payload_len9-0.366packet_len9-0.349packet_len5-0.347payload_len5-0.288interval1...
              -0.357interval3+0.351packet_len7+0.348payload_len7-0.324interval5+0.313interval7...
              0.454interval6-0.424interval5-0.401interval1+0.262interval9+0.256packet_len4...
              -0.402interval7-0.388payload_len6-0.387packet_len6-0.357interval8-0.26sport...
              -0.597interval9+0.443packet_len10-0.427packet_len2-0.239packet_len3-0.209packet_len6...
              0.865interval2+0.178interval3+0.168packet_len10+0.149packet_len4-0.146payload_len9...
              -0.751sport+0.346interval3+0.286dip+0.216interval7-0.178packet_len2...
              -0.434interval3+0.425interval2+0.315payload_len9+0.312packet_len9-0.311sport...
</code></p>

<p>降维之后的聚类结果</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-04-20-a-scratch-of-feature-selection-in-traffic-classification/reduction1.png?raw=true" alt="降维之后的聚类结果" title="降维之后的聚类结果" /></p>

<p>分析，由于降维特征减少太多，走势已经不太吻合。</p>

<h6 id="section-2">卡方分布提取特征算法</h6>

<p>对于这个算法，这里只给出特征选取的结果</p>

<p><code>text
Ranked attributes:
157.0864   12 interval7
145.6184   22 packet_len8
130.1739   20 packet_len6
123.7833    6 interval1
123.0833    9 interval4
118.16     32 payload_len8
108.7528   34 payload_len10
108.5061   24 packet_len10
103.1333   18 packet_len4
 92.3818   11 interval6
 85.5423   23 packet_len9
 81.0409   19 packet_len5
 61.9733   28 payload_len4
 42        31 payload_len7
 42        29 payload_len5
 42        30 payload_len6
 42        21 packet_len7
 39.4135   16 packet_len2
 39.4135   17 packet_len3
 38.4      14 interval9
 37.9167    2 sport
 36.9542    3 dip
 33.9      10 interval5
  0         5 protocal
  0        33 payload_len9
  0         4 dport
  0         1 sip
  0        25 payload_len1
  0        15 packet_len1
  0        13 interval8
  0         8 interval3
  0         7 interval2
  0        26 payload_len2
  0        27 payload_len3
</code></p>

<p>被取消的特征同样是那11个特征，只是排序结果不一样了。
所以理所当然kmeans实验室一致的。
Kmeans实验和实验1一样</p>

<p>最终选取的特征集合为23个</p>

<h4 id="section-3">参考</h4>

<p>[1]. Jamil, H.A., et al., Selection of On-line Features for Peer-to-Peer Network Traffic Classification, in Recent Advances in Intelligent Informatics. 2014, Springer. p. 379-390.</p>

<p>[2]. Zhen, L. and L. Qiong, A new feature selection method for internet traffic classification using ml. Physics Procedia, 2012. 33: p. 1338-1345.</p>

<p>[3]. Moore, A.W. and D. Zuev. Internet traffic classification using bayesian analysis techniques. in ACM SIGMETRICS Performance Evaluation Review. 2005: ACM.</p>

<p>[4]. Dash, M. and P.W. Koot, Feature selection for clustering, in Encyclopedia of database systems. 2009, Springer. p. 1119-1125.</p>

<p>[5]. 统计学习方法。李航</p>

<p>[6]. Mitra, P., C.A. Murthy and S.K. Pal, Unsupervised feature selection using feature similarity. IEEE transactions on pattern analysis and machine intelligence, 2002. 24(3): p. 301-312.</p>

]]></content>
  </entry>
  
</feed>
