
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Machine Learning :linear regression - 曾经渐行远，未免心戚戚</title>
  <meta name="author" content="Aluen King Lee">

  
  <meta name="description" content="机器学习中，总体来说是分为两类问题： 1.有监督的学习方法
2.无监督的学习方法 其他是这两者的综合，比如说半监督学习方法，强化学习（这个还未接触过）。 本文呢，先从有监督的学习方法开始讲起，主要是记载学习过程中个人认为最重要的地方。 对于监督学习中的两类问题，或者说三类吧，分别是：回归问题， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.aluenkinglee.com/blog/2014/01/05/machine-learning-linear-regression">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="曾经渐行远，未免心戚戚" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.useso.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.useso.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.useso.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
          MathJax.InputJax.TeX.prefilterHooks.Add(function (data) {
                  data.math = data.math.replace(/^\s*<!\[CDATA\[\s*((?:\n|.)*)\s*\]\]>\s*$/m,"$1");
                    });
          });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">曾经渐行远，未免心戚戚</a></h1>
  
    <h2>A blogging framework for hackers.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.aluenkinglee.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Machine Learning :linear Regression</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-01-05T12:00:00-08:00" pubdate data-updated="true">Jan 5<span>th</span>, 2014</time>
        
        
          | <a href="#comments">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>机器学习中，总体来说是分为两类问题：</p>

<p>1.有监督的学习方法
2.无监督的学习方法</p>

<p>其他是这两者的综合，比如说半监督学习方法，强化学习（这个还未接触过）。</p>

<p>本文呢，先从有监督的学习方法开始讲起，主要是记载学习过程中个人认为最重要的地方。</p>

<p>对于监督学习中的两类问题，或者说三类吧，分别是：回归问题，分类问题和标注问题（tagging）。后面这个很有意思，不过在这里现说一下回归和分类的区别，假如我们要
做一个连续变量的预测，比如说房价的预测，或者明日气温的预测，都是属于回归问题；而对于离散变量的预测，比如判断一个病人是否得了癌症，良性还是恶心，则是一个明显的分
类问题。</p>

<p>接下来的文章，大概是对Andrew Ng视频的一个简单的总结，会结合变成实例（octave和C++）来插叙。
<!--more--></p>

<h3 id="section">线性回归</h3>

<p>好吧，先从一个简单的例子讲起，假设我们要为一个房子售价做个数学模型，价格和什么有关系？当然因素很多，比如房间的大小，离商业区的距离，嗯，房子几坪，奥，看起来不是
个简单事儿～，那好吧，遵循我们先从最简单做起的原则，现假设相同尺寸的房子价格和城市人口多少有关系，其他的先抛到一边去，我喜欢做甩手掌柜==
，你看这很合理！北京上海的房子价格能和三四线城市的比么=。=</p>

<p>那么好，我们会看到下面这个图！图先不上！！！假设你装了octave，并执行ex1的话就会看到它的！！</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-01-05-machine-learning-linear-regression/linear_regression_f1.png?raw=true" alt="价格-人口关系图" title="价格-人口关系图" /></p>

<p>在那之前，先让我们约定几个问题，恩恩：</p>

<h5 id="section-1">注释</h5>
<ul>
  <li>$m$ ：是训练实例的个数</li>
  <li>$x$ ：是输入的特征向量,很有可能是这样子：$x=(x_1,…,x_k)$</li>
  <li>$y$ ：是输出结果</li>
  <li>$(x,y)$ ：是一个训练实例</li>
  <li>$(x^{(i)},y^{(i)})$ ：表示第i个训练样例</li>
</ul>

<p>好了，让我们接着开始吧。那我们应该如何表示我们的假设（hypothesis）呢？既然只有一个变量，这样表示好了：</p>

<script type="math/tex; mode=display">
h_\theta(x) = \theta_0 + \theta_1x_1 ， \Theta={ (\theta_0,\theta_1) }
</script>

<p>那应该如何选择参数$\theta$呢？机器学习不就是干这活的么=。=</p>

<p>直观的感受就是：“嘿，干嘛不用LMS最小二乘法？无脑流，简单又实惠！统计课上的入门案例。。”就他了。。。</p>

<p>所以，总结如下：</p>

<p>假设：</p>

<script type="math/tex; mode=display">
h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 ， 
\Theta = \left( \begin{array}{c}
        \theta_0 \\
        \theta_1
        \end{array} \right), 
x = (x_0,x_1),
x_0 \equiv 1 \\
h_\theta(x) = x \cdot \Theta
</script>

<p>费用函数：</p>

<script type="math/tex; mode=display">
J{(\Theta)}=\frac{1}{2m} \sum\limits_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)}
\right)^2 
</script>

<p>目标：</p>

<script type="math/tex; mode=display">
\min\limits_{\Theta} J{(\Theta)}
</script>

<p>回想下我们学过的数学知识吧，给定一个函数，求函数的最值，导数？梯度？那一套东西想起来了吧，OK。那好办了。要是还不是很清楚，那看一下<a href="http://zh
.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6">梯度</a>以及<a href="http://zh.wikipedia.org/wiki/%
E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">梯度下降法</a>在此就不罗嗦了。Andrew
Ng在视频中讲的很形象，只要我们沿着山最陡的方向向下走，就会有可能找到最小值，翻译成数学语言就是沿着梯度相反的方向$- \nabla F(x)$,
就可以下降最快。（我们不是要找最小值么，当然是水往低处流！所以就是负值了）</p>

<h4 id="section-2">梯度下降法</h4>

<p>选定了回归模型，那就要确定参数$\Theta$了，$\Theta$只有在$J{(\Theta)}
$最小的情况下才能确定，所以问题归结为了求极小值的问题，梯度下降法是个不错的选择。当然，它会遇到找到的值只是个局部最小值。</p>

<p>这是示意图：</p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-01-05-machine-learning-linear-regression/linear_regression_f4.png?raw=true" alt="最小值" /></p>

<p><img src="https://github.com/aluenkinglee/aluenkinglee.github.io/blob/source/source/images/2014-01-05-machine-learning-linear-regression/linear_regression_f5.png?raw=true" alt="局部极小值" /></p>

<p>流程如下：</p>

<ol>
  <li>对$\Theta$赋予初始值，可随机，可为零向量。</li>
  <li>同步改变$\Theta$值，使得$J{(\Theta)}$沿着梯度下降的方向走，直到学习曲线平滑，也就是收敛。</li>
</ol>

<p>用公式来描述就是,对于$j=1$和$j=0$，同时重复以下操作，直到$J{(\Theta)}$收敛。</p>

<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}
J{(\theta_0,\theta_1) } \\
\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^m 
\left(h_\theta(x^{(i)})-y^{(i)}
\right) \cdot x_j^{(i)}
</script>

<p>这是octave实现，向量形式,代码<a href="https://github.com/aluenkinglee/mlclass/blob/master/
mlclass-ex1/gradientDescent.m">详见</a></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="octave"><span class="line"><span class="k">function</span><span class="w"> </span>[theta, J_history] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescent</span><span class="p">(</span>X, y, theta, alpha, num_iters<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="c">%GRADIENTDESCENT Performs gradient descent to learn theta</span>
</span><span class="line"><span class="c">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by </span>
</span><span class="line"><span class="c">%   taking num_iters gradient steps with learning rate alpha</span>
</span><span class="line">
</span><span class="line"><span class="c">% Initialize some useful values</span>
</span><span class="line"><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>
</span><span class="line"><span class="n">J_history</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">num_iters</span><span class="p">,</span> 1<span class="p">);</span>
</span><span class="line"><span class="k">for</span> <span class="n">iter</span> <span class="p">=</span> 1<span class="p">:</span><span class="n">num_iters</span>
</span><span class="line">    <span class="c">%theta1 = theta(1) - alpha * X(:,1)&#39; *(X * theta - y) / m;</span>
</span><span class="line">    <span class="c">%theta2 = theta(2) - alpha * X(:,2)&#39; *(X * theta - y) / m;</span>
</span><span class="line">    <span class="c">%theta = [theta1; theta2]</span>
</span><span class="line">    <span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">));</span>
</span><span class="line">    <span class="c">% Save the cost J in every iteration    </span>
</span><span class="line">    <span class="n">J_history</span><span class="p">(</span><span class="n">iter</span><span class="p">)</span> <span class="p">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">);</span>
</span><span class="line"><span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>对应的C++实现，向量形式，代码<a href="https://github.com/aluenkinglee/mlclass/blob/master/mlclass
-ex1/gradientDescent.cpp">详见</a></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="cpp"><span class="line"><span class="cp">#include &quot;gradientDescent.h&quot;</span>
</span><span class="line"><span class="cp">#include &quot;computeCost.h&quot;</span>
</span><span class="line"><span class="k">using</span> <span class="k">namespace</span> <span class="n">mlclass</span><span class="o">::</span><span class="n">ex1</span><span class="p">;</span>
</span><span class="line"><span class="k">namespace</span> <span class="n">mlclass</span><span class="p">{</span>
</span><span class="line"><span class="k">namespace</span> <span class="n">ex1</span><span class="p">{</span>
</span><span class="line">    <span class="c1">//Performs gradient descent to learn theta</span>
</span><span class="line">    <span class="n">mat</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">mat</span> <span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="n">y</span><span class="p">,</span> <span class="n">mat</span><span class="o">&amp;</span> <span class="n">theta</span><span class="p">,</span> <span class="kt">double</span> <span class="n">alpah</span><span class="p">,</span><span class="kt">long</span> <span class="n">num_inters</span><span class="p">){</span>
</span><span class="line">        <span class="c1">//number of training examples</span>
</span><span class="line">        <span class="kt">long</span> <span class="n">m</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">n_rows</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">mat</span> <span class="n">J_history</span> <span class="o">=</span> <span class="n">zeros</span><span class="o">&lt;</span><span class="n">mat</span><span class="o">&gt;</span><span class="p">(</span><span class="n">num_inters</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
</span><span class="line">        <span class="kt">long</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">        <span class="k">for</span> <span class="p">(;</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_inters</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">){</span>
</span><span class="line">            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpah</span><span class="o">/</span><span class="n">m</span><span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">t</span><span class="p">()</span><span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">));</span>
</span><span class="line">            <span class="n">J_history</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">        <span class="k">return</span> <span class="n">J_history</span><span class="p">;</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><strong>有一个事情需要说明一下</strong></p>

<p>梯度下降发的收敛速度比较慢，相比于直接用公式求解$\theta$来说，尤其是当m较小的时候，比如说$m&lt;10000$,
这个时候用公式求解$\theta$比较快，但是大于这个值之后，计算矩阵的逆是花费较大的，此时使用梯度下降法比较理想，而且可以做到分布式计算值，加快求解速度。</p>

<script type="math/tex; mode=display">
\Theta=(X^TX)^-1X^Ty
</script>

<p>关于线性回归就先到这，接下来会记述关于logistic回归等的文章。</p>

<blockquote>
  <p>reference</p>
</blockquote>

<p>1.<a href="https://class.coursera.org/ml-004/lecture">Machine Learning by Andrew Ng(1-2)</a></p>

<p>2.<a href="http://mohu.org/info/symbols/symbols.htm">常用数学符号的 LaTeX 表示方法</a></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Aluen King Lee</span></span>

      








  


<time datetime="2014-01-05T12:00:00-08:00" pubdate data-updated="true">Jan 5<span>th</span>, 2014</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/armadillo/'>Armadillo</a>, <a class='category' href='/blog/categories/c-plus-plus/'>C++</a>, <a class='category' href='/blog/categories/machine-learning/'>Machine-Learning</a>, <a class='category' href='/blog/categories/linear-regression/'>linear-regression</a>, <a class='category' href='/blog/categories/octave/'>octave</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/12/31/ke-ai-de-armadillo/" title="Previous Post: 可爱的Armadillo">&laquo; 可爱的Armadillo</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/02/24/c-plus-plus-de-hashtablena-xie-shi/" title="Next Post: C++的hashtable那些事">C++的hashtable那些事 &raquo;</a>
      
    </p>
  </footer>
</article>


  <section>
    <h1>Comments</h1>
    <div id="comments" aria-live="polite"><!-- Duoshuo Comment BEGIN -->
	<div class="ds-thread"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"aluenkinglee"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- Duoshuo Comment END --></div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/09/02/qian-tan-wei-bo-biao-qian-ju-he/">浅谈微博标签聚合</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/12/use-svm-to-classify-whether-a-person-is-good-or-bad-on-credit-using-sns-data/">Use Svm to Classify Whether a Person Is Good or Bad on Credit Using Sns Data</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/09/wei-pi-pei/">伪匹配</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/27/po-su-bei-xie-si-wen-ben-fen-lei/">朴素贝叶斯文本分类</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/03/lagrange-duality/">拉格朗日对偶</a>
      </li>
    
  </ul>
</section>




<section>
  <h1>My Douban</h1>
  <div>
  <!--添加到这-->
  <script type="text/javascript" src="http://www.douban.com/service/badge/aluenkinglee/?selection=random&amp;picsize=small&amp;show=dolist&amp;n=8&amp;cat=book&amp;columns=3"></script>
  </div>
</section> 
<section>
<h1>Recent Comments</h1>
<ul class="ds-recent-comments" data-num-items="10" data-show-avatars="0" data-show-time="0" data-show-title="0" data-show-admin="0" data-excerpt-length="18"></ul>

</section> 

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Aluen King Lee -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
